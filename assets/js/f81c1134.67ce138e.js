"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/ai-protocol-revolution","metadata":{"permalink":"/blog/ai-protocol-revolution","source":"@site/blog/2025-09-20-ai-protocols.md","title":"The AI Protocol Revolution: A Story of History Repeating Itself","description":"AI Protocol Revolution Banner","date":"2025-09-20T00:00:00.000Z","tags":[{"inline":true,"label":"mcp","permalink":"/blog/tags/mcp"},{"inline":true,"label":"a2a","permalink":"/blog/tags/a-2-a"},{"inline":true,"label":"agents","permalink":"/blog/tags/agents"}],"readingTime":18.525,"hasTruncateMarker":true,"authors":[{"name":"Mark Edmondson","title":"Founder","url":"https://sunholo.com/","imageURL":"https://code.markedmondson.me/images/gde_avatar.jpg","socials":{"github":"https://github.com/MarkEdmondson1234","linkedin":"https://www.linkedin.com/in/markpeteredmondson/"},"key":"me","page":null}],"frontMatter":{"title":"The AI Protocol Revolution: A Story of History Repeating Itself","authors":"me","tags":["mcp","a2a","agents"],"slug":"/ai-protocol-revolution","image":"blog/img/ai-protocols.png"},"unlisted":false,"nextItem":{"title":"Why GenAI Needs a Subconscious: Internal Monologues for your Cognitive Designs","permalink":"/blog/subconscious-genai"}},"content":"import AudioPlayer from \'@site/src/components/audio\';\\nimport CogFlow from \'@site/src/components/reactFlow\';\\nimport ProtocolComparison from \'@site/src/components/protocolComparison\';\\n\\n![AI Protocol Revolution Banner](img/ai-protocols.png)\\n\\nHere at Sunholo, we\'ve specialised in deploying GenAI applications for the past few years. Recently, when talking to new prospects we have noticed a trend: they show us their own internal chatbot, built at great expense just 18 months ago, and ask why it feels already outdated compared to ChatGPT or Gemini. Is there a better way to keep on the cutting edge but still keep your AI application bespoke? The answer takes us on a journey through web history, emerging protocols, and a future that\'s arriving faster than most realize.\\n\\n<AudioPlayer src=\\"https://storage.googleapis.com/sunholo-public-podcasts/The_Protocol_Wars__Why_Your_Custom_AI_Is_Failing_and_How_New_St.mp4\\" />\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n\\n## AI Web = Web 1.0 / 2.0 ?\\n\\nRemember Web 1.0? That nostalgic era of disparate hobby websites, the dot-com bubble, and the rise of search engines? One framing of that era could be that web-enabled companies were offering a new way to access their databases, transforming their data into HTML web portals.  This read-only access gave birth to the information superhighway, and we felt a boom (and bust) as that data was given in exchange for traffic and ad revenue.\\n\\nWeb 2.0 evolution occurred when companies started to let users WRITE as well as READ to those databases. Suddenly websites could update in real-time with user generated content: blog comments, tweets, social interactions. Facebook and others then built walled gardens around that data and monetised it with personalised feeds, selling user behaviour to companies for highly targeted advertising in exchange for enhanced communications with one another. \\n\\nThe AI evolution could be said to be following the same pattern, only accelerated. ChatGPT started with conversations with static models, with chat history context.  Then everyone got excited by vector embeddings and RAG for passing in their own data into the model\'s context window and prompts. Now users and 3rd parties can bring their own data, as Agentic AI reaches out for other sources. But there\'s a problem.\\n\\n## The Integration Nightmare\\n\\nMIT\'s recent study claimed that 95% of generative AI pilots fail to achieve rapid revenue acceleration:\\n\\n- [MIT report: 95% of generative AI pilots at companies are failing | Fortune](https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/) \\n\\nBut that tells only part of the story. What they don\'t say is why.  \\n\\nOne reason could be that self-build AI applications need to have unique user interfaces or unique data to be any better than the generic AI applications.  If you don\'t have either of those (preferebly both) then you are going to be going head-to-head with hyperscalers with more resources than you.  To be of value, your internal AI tools needed custom integration with internal data sources and custom UIs, and home-spun solutions can quickly become outdated as the rapid pace of AI abilities outpaces developer project time. \\n\\nWhat\'s the answer? Standards. It is the experience from thousands of AI deployments across the world that has enabled AI commmuity feedback necessary for standard protocols to emerge.\\n\\n## The Protocols Emerge (MCP & A2A)\\n\\nIn November 2024, Anthropic released the [Model Context Protocol (MCP)](https://www.anthropic.com/news/model-context-protocol). It addressed the emerging need for AI industry standards for what everyone was building, but in slightly different ways. MCP isn\'t particularly unique in its properties, much like HTTP wasn\'t. The value of a protocol comes only if and when it gains widespread adoption.\\n\\nAnthropic was the perfect source for this initiative. They\'re respected by developers for their model\'s coding capabilities but also neutral enough, being deployed across Google Cloud, AWS, and Azure, to be trusted by everyone.\\n\\nThen in April 2025, Google announced the Agent2Agent (A2A) protocol, backed by 50+ tech companies \\n\\n* [Announcing the Agent2Agent Protocol (A2A) - Google Developers Blog](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/). \\n\\nWhile MCP connects AI to tools and data, A2A defines how AI agents collaborate. It\'s the difference between giving workers tools and teaching them to work as a team.\\n\\nThe A2A protocol again would only be of value if it was not a vendor lock-in to Google, so it was deliberately kept vendor-neutral, transferred to the open source [Linux Foundation project](https://developers.googleblog.com/en/google-cloud-donates-a2a-to-linux-foundation/) and so is also endorsed by competitor AI companies such as AWS, Salesforce, ServiceNow, Microsoft and IBM. Like HTTP before it, A2A\'s value only emerges through universal adoption.\\n\\n<ProtocolComparison\\n  title=\\"From HTTP to A2A: How Protocols Shape Technology Eras\\"\\n  mode=\\"timeline\\"\\n  showLegend={false}\\n  items={[\\n    {\\n      name: \\"HTTP/HTML\\",\\n      year: \\"1990s\\",\\n      features: [\\"Read-only databases\\", \\"Static content\\", \\"Ad revenue model\\"],\\n      description: \\"Web 1.0 - Companies offered read-only access to databases in exchange for traffic\\",\\n      color: \\"#666\\",\\n      stats: { adoption: \\"Universal\\" }\\n    },\\n    {\\n      name: \\"SSL/HTTPS\\",\\n      year: \\"1995\\",\\n      features: [\\"Encrypted transactions\\", \\"Digital certificates\\", \\"E-commerce enabler\\"],\\n      description: \\"The first protocol to enable trust in online commerce - without it, no Web 2.0\\",\\n      color: \\"#4a5568\\",\\n      hideInComparison: true\\n    },\\n    {\\n      name: \\"3D Secure\\",\\n      year: \\"2001\\",\\n      features: [\\"Human authentication\\", \\"Fraud prevention\\", \\"Payment verification\\"],\\n      description: \\"Like AP2 for humans - proving you authorized that credit card transaction\\",\\n      details: \\"3D Secure (Verified by Visa, Mastercard SecureCode) authenticates online card payments. It\'s the Web 2.0 equivalent of AP2 - both solve \'How do we verify this transaction is authorized?\' for their respective actors.\\",\\n      color: \\"#dc2626\\",\\n      hideInComparison: true\\n    },\\n    {\\n      name: \\"Web 2.0 APIs\\",\\n      year: \\"2000s\\", \\n      features: [\\"Write access to databases\\", \\"User-generated content\\", \\"Walled gardens\\"],\\n      description: \\"Companies let users WRITE to databases, creating social platforms and targeted ads\\",\\n      color: \\"#999\\",\\n      stats: { adoption: \\"Mainstream\\" }\\n    },\\n    {\\n      name: \\"OAuth 2.0\\",\\n      year: \\"2012\\",\\n      features: [\\"Delegated access\\", \\"API economy\\", \\"Third-party apps\\"],\\n      description: \\"Enabled the app ecosystem - let services talk without sharing passwords\\",\\n      color: \\"#7c3aed\\",\\n      hideInComparison: true\\n    },\\n    {\\n      name: \\"ChatGPT Launch\\",\\n      year: \\"Nov 2022\\",\\n      features: [\\"AI goes mainstream\\", \\"Chat interface\\", \\"No code required\\"],\\n      description: \\"The moment AI became accessible to everyone - sparking the custom chatbot gold rush\\",\\n      details: \\"ChatGPT reached 100 million users in just 2 months, making it the fastest-growing consumer application in history. This sparked thousands of companies to build custom chatbots - many of which feel outdated just 18 months later.\\",\\n      links: [\\n        { title: \\"MIT Study: 95% of AI Pilots Fail\\", url: \\"https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/\\" }\\n      ],\\n      color: \\"#10b981\\",\\n      hideInComparison: true\\n    },\\n    {\\n      name: \\"GPT-4\\",\\n      year: \\"Mar 2023\\",\\n      features: [\\"Rapid improvement\\", \\"Multimodal\\", \\"Extended context\\"],\\n      description: \\"The pace of change that makes 18-month-old chatbots feel obsolete\\",\\n      color: \\"#10b981\\",\\n      hideInComparison: true\\n    },\\n    {\\n      name: \\"Google Bard/Gemini\\",\\n      year: \\"Mar 2023\\",\\n      features: [\\"Competition intensifies\\", \\"Free access\\", \\"Integration race\\"],\\n      description: \\"Big Tech enters - now everyone needs to keep up with multiple AI providers\\",\\n      color: \\"#4285f4\\",\\n      hideInComparison: true\\n    },\\n    {\\n      name: \\"MCP\\",\\n      year: \\"Nov 2024\\",\\n      features: [\\"Standard tool access\\", \\"No custom integrations\\", \\"Universal context\\"],\\n      description: \\"The integration nightmare ends - AI can now connect to any tool without custom code\\",\\n      details: \\"MCP provides a universal, open standard for connecting AI systems with data sources. Like USB-C for AI, it replaces fragmented integrations with a single protocol. Pre-built servers exist for Google Drive, Slack, GitHub, and more.\\",\\n      links: [\\n        { title: \\"Official MCP Announcement\\", url: \\"https://www.anthropic.com/news/model-context-protocol\\" },\\n        { title: \\"MCP Documentation\\", url: \\"https://modelcontextprotocol.io\\" },\\n        { title: \\"GitHub Repository\\", url: \\"https://github.com/modelcontextprotocol\\" }\\n      ],\\n      color: \\"#0066cc\\",\\n      stats: { adoption: \\"Growing\\", companies: \\"100+\\" }\\n    },\\n    {\\n      name: \\"A2A\\",\\n      year: \\"Apr 2025\\",\\n      features: [\\"Agent teamwork\\", \\"Cross-platform\\", \\"Vendor neutral\\"],\\n      description: \\"AI agents can now collaborate - the difference between tools and teams\\",\\n      details: \\"A2A enables AI agents built on diverse frameworks to communicate and collaborate. It complements MCP - while MCP connects agents to tools, A2A enables agent-to-agent collaboration. Now backed by 150+ organisations including Microsoft, IBM, and SAP.\\",\\n      links: [\\n        { title: \\"A2A Announcement Blog\\", url: \\"https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/\\" },\\n        { title: \\"A2A GitHub Repository\\", url: \\"https://github.com/a2aproject/A2A\\" },\\n        { title: \\"Getting Started with A2A\\", url: \\"https://codelabs.developers.google.com/intro-a2a-purchasing-concierge\\" },\\n        { title: \\"Google Agentspace\\", url: \\"https://cloud.google.com/products/agentspace\\" }\\n      ],\\n      color: \\"#00aa44\\",\\n      stats: { backers: \\"50+\\", status: \\"Active\\" }\\n    },\\n    {\\n      name: \\"AP2\\",\\n      year: \\"Sep 2025\\",\\n      features: [\\"Agent commerce\\", \\"Micropayments\\", \\"Trust verification\\"],\\n      description: \\"The agent economy arrives - AI can hire AI, pay per use, no humans required\\",\\n      details: \\"AP2 uses cryptographically-signed \'Mandates\' to prove user authorisation for transactions. Supports credit cards, crypto, and bank transfers. Partners include American Express, Mastercard, PayPal, Coinbase, and 60+ others.\\",\\n      links: [\\n        { title: \\"AP2 Announcement\\", url: \\"https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol\\" },\\n        { title: \\"How AP2 Works\\", url: \\"https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol\\" }\\n      ],\\n      color: \\"#ff9900\\",\\n      stats: { partners: \\"60+\\", includes: \\"Mastercard, PayPal\\" }\\n    }\\n  ]}\\n/>\\n\\n## The Next Step: AI Commerce\\n\\nGoogle\'s recent Agent Payments Protocol (AP2) is an extension to A2A, developed with 60+ organisations including Mastercard and PayPal \\n\\n- [Announcing Agent Payments Protocol (AP2) | Google Cloud Blog](https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol), \\n\\nAP2 adds what might be the most transformative element. Entirely new business models can now emerge based on the value added by each individual agent, and those negotiations on buying and selling can also be done by the AP2 enabled agents with or without human intervention.\\n\\nPicture this: Your market research agent needs real-time data. It automatically purchases $0.02 worth of web scraping from a data harvester agent, then pays $0.05 to a sentiment analysis specialist for processing, $0.01 to a fact-checker agent for verification, and finally $0.03 to a visualisation agent for charts. Hundreds of micro-transactions per minute, agents bidding for work, specialised models competing on price and quality. No humans involved, just AI agents trading skills and knowledge in a digital marketplace. The agent economy isn\'t coming; it\'s being built right now.\\n\\n### How A2A and MCP Work Together\\n\\nHere\'s a practical example of how these protocols interact in a real-world scenario: A company\'s research agent needs to analyze market data across multiple languages and data sources:\\n\\n<CogFlow\\n  title=\\"A2A and MCP Protocol Interaction\\"\\n  nodes={[\\n    { id: \'1\', data: { label: \'\ud83d\udc64 User Request\', hasInput: false, hasOutput: true }, position: { x: 400, y: 20 }, type: \'customNode\' },\\n    { id: \'2\', data: { label: \'\ud83c\udfe2 Research Agent [A2A]\', hasInput: true, hasOutput: true, backgroundColor: \'#d4f4dd\', borderColor: \'#00aa44\' }, position: { x: 400, y: 120 }, type: \'customNode\' },\\n    { id: \'3\', data: { label: \'\ud83d\udcca Market Agent [A2A]\', hasInput: true, hasOutput: true, backgroundColor: \'#d4f4dd\', borderColor: \'#00aa44\' }, position: { x: 200, y: 240 }, type: \'customNode\' },\\n    { id: \'4\', data: { label: \'\ud83c\udf10 Translation Agent [A2A]\', hasInput: true, hasOutput: true, backgroundColor: \'#d4f4dd\', borderColor: \'#00aa44\' }, position: { x: 600, y: 240 }, type: \'customNode\' },\\n    { id: \'5\', data: { label: \'\ud83d\udcc8 Bloomberg [MCP]\', hasInput: true, hasOutput: true, backgroundColor: \'#e6e6e6\', borderColor: \'#666\' }, position: { x: 55, y: 450 }, type: \'customNode\' },\\n    { id: \'6\', data: { label: \'\ud83d\uddc4\ufe0f Database [MCP]\', hasInput: true, hasOutput: true, backgroundColor: \'#e6e6e6\', borderColor: \'#666\' }, position: { x: 200, y: 380 }, type: \'customNode\' },\\n    { id: \'7\', data: { label: \'\ud83d\udc8e Gemini [LLM]\', hasInput: true, hasOutput: true, backgroundColor: \'#f0e6ff\', borderColor: \'#9933cc\' }, position: { x: 350, y: 320 }, type: \'customNode\' },\\n    { id: \'8\', data: { label: \'\ud83e\udde0 Claude [LLM]\', hasInput: true, hasOutput: true, backgroundColor: \'#f0e6ff\', borderColor: \'#9933cc\' }, position: { x: 550, y: 380 }, type: \'customNode\' },\\n    { id: \'9\', data: { label: \'\ud83d\udcc1 Drive [MCP]\', hasInput: true, hasOutput: true, backgroundColor: \'#e6e6e6\', borderColor: \'#666\' }, position: { x: 750, y: 380 }, type: \'customNode\' },\\n    { id: \'10\', data: { label: \'\ud83d\udcb3 Payments [AP2]\', hasInput: true, hasOutput: true, backgroundColor: \'#ffe6cc\', borderColor: \'#ff9900\' }, position: { x: 400, y: 520 }, type: \'customNode\' },\\n    { id: \'11\', data: { label: \'\ud83d\udccb Report\', hasInput: true, hasOutput: false }, position: { x: 400, y: 620 }, type: \'customNode\' },\\n  ]}\\n  edges={[\\n    { id: \'e1-2\', source: \'1\', target: \'2\', label: \'\', animated: true, style: { stroke: \'#0066cc\', strokeWidth: 2 } },\\n    { id: \'e2-3\', source: \'2\', target: \'3\', label: \'A2A\', animated: true, style: { stroke: \'#00aa44\', strokeWidth: 2 } },\\n    { id: \'e2-4\', source: \'2\', target: \'4\', label: \'A2A\', animated: true, style: { stroke: \'#00aa44\', strokeWidth: 2 } },\\n    { id: \'e3-5\', source: \'3\', target: \'5\', label: \'MCP\', animated: false, style: { stroke: \'#666\', strokeDasharray: \'5,5\' } },\\n    { id: \'e3-6\', source: \'3\', target: \'6\', label: \'MCP\', animated: false, style: { stroke: \'#666\', strokeDasharray: \'5,5\' } },\\n    { id: \'e3-7\', source: \'3\', target: \'7\', label: \'\', animated: false, style: { stroke: \'#9933cc\', strokeDasharray: \'3,3\' } },\\n    { id: \'e4-8\', source: \'4\', target: \'8\', label: \'\', animated: false, style: { stroke: \'#9933cc\', strokeDasharray: \'3,3\' } },\\n    { id: \'e4-9\', source: \'4\', target: \'9\', label: \'MCP\', animated: false, style: { stroke: \'#666\', strokeDasharray: \'5,5\' } },\\n    { id: \'e3-10\', source: \'3\', target: \'10\', label: \'$0.05\', animated: false, style: { stroke: \'#ff9900\' } },\\n    { id: \'e4-10\', source: \'4\', target: \'10\', label: \'$0.10\', animated: false, style: { stroke: \'#ff9900\' } },\\n    { id: \'e10-11\', source: \'10\', target: \'11\', label: \'\', animated: true, style: { stroke: \'#0066cc\', strokeWidth: 2 } },\\n  ]}\\n  height=\\"700px\\"\\n/>\\n\\n\\nIn this example:\\n- **A2A Protocol** handles agent-to-agent communication: The company\'s research agent discovers and coordinates with external specialist agents\\n- **MCP Protocol** connects agents to tools and data: Each agent uses MCP to access databases, APIs, and file systems\\n- **AP2 Protocol** manages micropayments: External agents charge small fees automatically without human intervention\\n- **LLMs** provide the intelligence: Agents use various models (Claude, GPT, Gemini) for their specific tasks\\n\\nThe beauty is that the company\'s research agent doesn\'t need to know how the translation agent works internally, or which LLM it uses. It just sends an A2A task request and receives results. Similarly, agents don\'t need custom integrations for each tool\u2014MCP provides a standard interface to everything from databases to SaaS APIs.\\n\\n## The Living Laboratory\\n\\n[Google Agentspace](https://cloud.google.com/products/agentspace), already deployed at companies like Wells Fargo, KPMG, and Nokia, shows what this A2A led infrastructure looks like in practice. It\'s one of Google Cloud\'s fastest-growing products ever.  See a demo video below - the Agent Gallery is all enabled by A2A:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/V-r0WjXJhL8?si=NZVJhZewYq5RXCNK\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\nThat dusty SharePoint archive from 2015? Suddenly searchable alongside this morning\'s Slack conversations. The rigid SAP system that took six months to integrate? Now accessible to AI agents without touching a line of code. Agentspace leverages A2A and has announced an A2A marketplace for seamless integration with its platform from all: including non-Google vendors, would-be rivals and open-source competitors. \\n\\nAs proof of that commitment to avoid vendor lock-in, the A2A protocol works with all major AI frameworks such as Langchain and Pydantic AI but also including Google\'s own ADK, demonstrating the power of unified AI access to enterprise data.\\n\\n## Embrace inflexible standards to be flexible in delivery\\n\\nAfter the first two years post Chat-GPT in the AI trenches, we are seeing a pattern for the AI early adopters. A lot of companies that were cutting edge in 2023 \u2014 the ones who built chatbots plus RAG for internal document search \u2014 are now stuck. They can\'t keep up with the pace of AI improvements by the hyperscalers. Every time Gemini, Claude or OpenAI releases an update such as artifacts, thinking tokens or code execution, they face months of integration work to match it with less resources.\\n\\nThis is the [bitter lesson of AI](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) applied to AI infrastructure. As Rich Sutton writes: \\n\\n> \\"The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.\\" Over a long enough timespan, specialised solutions lose to general approaches that leverage computation. \\n\\nBut we\'re learning a corollary: rigid infrastructure becomes tomorrow\'s technical debt, fast.  For example, the \\"chat-to-PDF\\" features of 2023 usually involved data pipelines that are now redundant in modern AI deployments since that feature is now just one AI API call away. The PDF parsing pipelines that were developed and worked in 2023 are now actually hindering performance of PDF understanding if unable to use the new vision or file multi-modal abilities of modern AIs.\\n\\nThe solution isn\'t just technical, but philosophical. Build for change, not features. Every AI component needs to be independently replaceable, like swapping batteries rather than rewiring your house. When OpenAI, Google or Claude releases a new model or feature next month (and they will), you should be able to adapt to it within hours, especially in this AI-code assisted future. If you get it right, then your application automatically improves in lock step with the AI models underneath it.  The restrictions of complying with protocols give you the freedom to be flexible.\\n\\nGet it wrong however, and you are stuck with old features that your staff do not use in favour of \\"shadow AI\\" being used via personal phones or bypassing VPN controls.  Why that matters?  Those interactions are incredibly valuable in assessing what your colleagues are actually working on, including what drives are important for your company. Passing that to a 3rd party and not having those AI conversations available for your own review gives the keys to your business improvement elsewhere, out of your control.\\n\\n## The Non-Human Web Emerges\\n\\nHumans are becoming less likely to be direct consumers of web data. We may soon reach peak human web traffic, with the proportion of human traffic ever declining from now on in favour of AI bot traffic.\\n\\n![](img/web-traffic-decline.png)\\n*Chart from https://www.economist.com/business/2025/07/14/ai-is-killing-the-web-can-anything-save-it*\\n\\nGoogle\'s AI Overviews now appear in over 35% of U.S. searches, with some sites reporting traffic drops of up to 70%. According to Pew Research, just 8% of users who encountered an AI summary clicked through to a traditional link\u2014half the rate of those who didn\'t \\n\\n* [Pew Research Confirms Google AI Overviews Is Eroding Web Ecosystem | Search Engine Journal](https://www.searchenginejournal.com/pew-research-confirms-google-ai-overviews-is-eroding-web-ecosystem/551825/).\\n\\nThink about your own behavior. How often do you ask ChatGPT or Google\'s AI for information instead of visiting websites yourself? Now multiply that by billions of users and add AI agents that never sleep, never get tired, and can visit thousands of sites per second.\\n\\nCurrently, there\'s a booming market for web scrapers\u2014tools that help AI read websites designed for humans. But we propose that this is transitional, like mobile websites before responsive design. The same databases that generate HTML for humans are able to generate tailored AI responses via MCP and A2A directly to AI agents, without the messy parsing of HTML.\\n\\nAnother current alternative is [/llm.txt](https://llmstxt.org/) which AI-savvy websites are using, that simply do the parsing for the AI without the need of going via a HTML parsing tool.  It strips away all the messy HTML and offers text only content for hungry AI to process.  We\'re building a parallel web for machines.\\n\\n## The Business Model Breaking Point\\n\\nThe impact on media and content businesses is existential. Cloudflare (who can see ~20% of total web traffic, 63 million requests per second) data shows that for every visitor Anthropic refers back to a website, its crawlers have already visited tens of thousands of pages. OpenAI\'s crawler scraped websites 1,700 times for every referral, while Google\'s ratio was 14-to-1 \\n\\n* [The crawl-to-click gap: Cloudflare data on AI bots, training, and referrals | Cloudflare](https://blog.cloudflare.com/crawlers-click-ai-bots-training/).\\n\\n\\n![](img/bot-traffic-cloudflare.png)\\n\\n*Cloudflare tracks human vs bot traffic in its radar dashboard https://radar.cloudflare.com/bots*\\n\\nThis unsustainable imbalance led to a radical response. In July 2025, Cloudflare announced it could block AI crawlers by default and launched [\\"Pay Per Crawl\\"](https://blog.cloudflare.com/introducing-pay-per-crawl/) \u2014 a solution where publishers can charge AI companies for each page crawled. It\'s the first serious attempt to create a new business model for the AI era, where content isn\'t just consumed but compensated.  Here the current HTTP protocol is invoked, using an obscure HTTP access code 402 (as opposed to 404, 200 etc) indicating \\"Payment Required\\".\\n\\n## The Human Question\\n\\nWhat happens to humans in this new world? Beautiful showroom websites will likely remain as spaces for inspiration and brand experience. But the messy functionality of websites: complex forms, comparison shopping, detailed research, could likely shift to AI agents working in the background.\\n\\nSome companies are betting on the \\"everything app\\" approach. OpenAI and X.com seem to envision users never leaving their platforms, consuming all content through a single AI interface. It\'s Web 2.0\'s walled gardens taken to their logical extreme.\\n\\nHow does this impact web analytics? E-commerce conversion rates? Media websites that survived on impression-based advertising now face an extinction-level event.\\n\\n## The Privacy Revolution Returns\\n\\nThere\'s a twist in our story that harks back to Web 2.0\'s original promise: users controlling their own data.\\n\\n![](img/private-mcp.png)\\n\\nWhat if individuals maintained their own A2A or MCP servers? All your purchase history, preferences, relationships, and interests in one place, under your control. You\'d grant selective access to services in exchange for better experiences\u2014verified, accurate profiles instead of the creepy tracking and guessing that defines today\'s web.\\n\\nThe protocols make this technically feasible today. The question is whether a post-GDPR population, increasingly aware of privacy violations, will demand it. Could user-controlled AI servers become the next revolution?\\n\\nAnd this time around, the privacy stakes are higher.  People were worried about Cambridge Analytica interpreting signals from user behaviour via web traffic analytics potentially influencing elections via paid ads.  Thats insignficiant next to the potential harm that could be done by an AI that is super persuasive, with access to all your thoughts, dreams and desires typed into its chat box.\\n\\n## The Inflection Point\\n\\nWe\'re witnessing the end of AI\'s wild west phase. Standards are emerging. The organisations recognising this shift\u2014building for tomorrow\'s pace of change rather than today\'s requirements\u2014will define the next era.\\n\\nThe future isn\'t about having the best AI. It\'s about having AI that can collaborate with everyone else\'s AI, upgrade without breaking, experiment without committing, and respect user privacy and control.\\n\\nThe protocols are here. The early adopters are moving. The business models are being rewritten\u2014from impression-based advertising to pay-per-crawl, from human web traffic to agent economies. The question isn\'t whether to embrace these standards, but whether you\'ll be part of the 5% that succeed or the 95% still trying to maintain custom integrations that were obsolete before they were finished.\\n\\nHistory doesn\'t repeat, but it rhymes. The web\'s evolution from chaos to standards to walled gardens to user control could be playing out again, just faster and with artificial minds as the primary actors.  Will user privacy follow the same path, or do we have a chance to reshape the balance between user and company, to preserve human dignity? The stakes look to be higher this time around.\\n\\nWhere does your organisation fit in this story?\\n\\n---\\n\\n*Want to discuss how to navigate this transition? Reach out at multivac@sunholo.com or visit [www.sunholo.com](https://www.sunholo.com)*"},{"id":"/subconscious-genai","metadata":{"permalink":"/blog/subconscious-genai","source":"@site/blog/2024-10-22-subconscious-genai.md","title":"Why GenAI Needs a Subconscious: Internal Monologues for your Cognitive Designs","description":"A cognitive design I\u2019ve come across recently apes the subconscious messages we have in our own brains, as distinct from inner monologue or stuff we say. Referencing our own way of thinking has revealed to me insight about how to improve GenAI functionality, as well as revealing back to me new insights into how we ourselves think.  I\'m a distinct amateur in neuroscience, so I hope if I blog this someone more informed could perhaps comment on the approach outlined below, but I am finding it a very useful technique.","date":"2024-10-22T00:00:00.000Z","tags":[{"inline":true,"label":"agents","permalink":"/blog/tags/agents"},{"inline":true,"label":"cognitive-design","permalink":"/blog/tags/cognitive-design"}],"readingTime":16.455,"hasTruncateMarker":true,"authors":[{"name":"Mark Edmondson","title":"Founder","url":"https://sunholo.com/","imageURL":"https://code.markedmondson.me/images/gde_avatar.jpg","socials":{"github":"https://github.com/MarkEdmondson1234","linkedin":"https://www.linkedin.com/in/markpeteredmondson/"},"key":"me","page":null}],"frontMatter":{"title":"Why GenAI Needs a Subconscious: Internal Monologues for your Cognitive Designs","authors":"me","tags":["agents","cognitive-design"],"image":"https://dev.sunholo.com/assets/images/subconscious-afec73da2d5a115ecd2c3c16776300e3.png","slug":"/subconscious-genai"},"unlisted":false,"prevItem":{"title":"The AI Protocol Revolution: A Story of History Repeating Itself","permalink":"/blog/ai-protocol-revolution"},"nextItem":{"title":"Dynamic UIs in Markdown using GenAI, React Components and MDX","permalink":"/blog/dynamic-output-mdx"}},"content":"import AudioPlayer from \'@site/src/components/audio\';\\nimport CogFlow from \'@site/src/components/reactFlow\';\\n\\n![](img/subconscious.png)\\n\\nA cognitive design I\u2019ve come across recently apes the subconscious messages we have in our own brains, as distinct from inner monologue or stuff we say. Referencing our own way of thinking has revealed to me insight about how to improve GenAI functionality, as well as revealing back to me new insights into how we ourselves think.  I\'m a distinct amateur in neuroscience, so I hope if I blog this someone more informed could perhaps comment on the approach outlined below, but I am finding it a very useful technique.\\n\\n> See this post about how I define [Cognitive Design for GenAI architecture](/blog/cognitive-design) \\n\\nFor this explanation, I break down cognition messaging into three modes:\\n\\n:::note[Cognition Messaging]\\n* **What we say to others** - e.g. us talking.  I attribute this in GenAI to a bot\'s output, like chat text.\\n* **Our inner monologue** - e.g. using our language for internal thoughts.  I attribute this to logging messages passed within the GenAI functions but not exposed to the end user.\\n* **Our subconscious** - e.g. thoughts we are not aware of, but influence our thoughts.  I attribute this to internal logging and messages within a GenAI function, that are not surfaced to an outer agent.  \\n:::\\n\\nI believe the messages passed around within a cognitive design can be broken out into the sub-categories above, and that can help us design better performing systems.  This seems to become important once one starts to work with asynchronous, parallel calls to GenAI models, which again I think may be because that is more akin to how human brains work, as opposed to sequential, one call at a time API requests we start with when first getting to know GenAI models.\\n\\n<AudioPlayer src=\\"https://storage.googleapis.com/sunholo-public-podcasts/Subconscious%20messages%20in%20GenAI.wav\\" />\\n\\n\x3c!-- truncate --\x3e`\\n\\n## Cognitive design and agent orchestration\\n\\nUsing the above approach, I\'ve created bots that takes in various contexts and responds well to a variety of questions. It responds quickly, but as it is answering internal monologue influences and evolves the answer as its writing, until the reply ends with a reflective summary on everything it has just said. Note this is distinct from prompting techniques such as [ReACT](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/) or [Chain of Thought](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/), which rely on a sequential, single API call.  A parallel approach for calling GenAI models means working at a more [GenOps](https://www.sunholo.com) or data engineering level, aggregating API requests to GenAI models and orchestrating their parrallel returns via async or microservice patterns.\\n\\nFor a while now I\'ve been thinking about how I could apply the principles in [Daniel Kahneman\'s Thinking Fast and Slow](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555) book, which introduces \\"System 1\\" (fast) and \\"System 2\\" (slow) thinking.  Both ways of thinking have their usefulness, and making convincing GenAI bots that incorporate the same feels like a good route to making better bots.\\n\\nI\u2019m not a big believer in \\"AGI\\" if defined as a machine that can create novel new reasoning not in its training set or possessing internal qualia, but I do think large language models are going to be fantastically useful in surfacing all of human expression. We already see how metacognition techniques seem to help performance of agents at a prompt level (e.g. chain of thought). If copying mental patterns such as System 1/2 visibly help a silicon based agent, it\u2019s a fascinating question why thats the case and worth exploring. \\n\\n## Inner monologue vs Subconscious Messaging\\n\\nI\u2019ve come across the need for subconscious messages when dealing with orchestrating several models in parallel which then feed into an orchestrator agent taking their output and summarising it.\\n\\nAn example cognitive design is shown below (using a bit of [React in MDX](/blog/dynamic-output-mdx) I learnt in my last post)\\n\\n<CogFlow\\n  title=\\"\\"\\n  nodes={[\\n    { id: \'1\', data: { label: \'User Request\', hasInput: false, hasOutput: true }, position: { x: 250, y: 50 }, type: \'customNode\' },\\n    { id: \'2\', data: { label: \'Orchestrator Agent\', hasInput: true, hasOutput: true }, position: { x: 250, y: 150 }, type: \'customNode\' },\\n    { id: \'3\', data: { label: \'Google Search Bot\', hasInput: true, hasOutput: true }, position: { x: 100, y: 250 }, type: \'customNode\' },\\n    { id: \'4\', data: { label: \'Database Query Agent\', hasInput: true, hasOutput: true }, position: { x: 400, y: 250 }, type: \'customNode\' },\\n    { id: \'5\', data: { label: \'SQL Agent\', hasInput: true, hasOutput: false }, position: { x: 500, y: 350 }, type: \'customNode\' },\\n    { id: \'6\', data: { label: \'Streaming Output\', hasInput: true, hasOutput: true }, position: { x: 250, y: 450 }, type: \'customNode\' },\\n    { id: \'7\', data: { label: \'User Respoonse\', hasInput: true, hasOutput: false }, position: { x: 250, y: 550 }, type: \'customNode\' },\\n  ]}\\n  edges={[\\n    { id: \'e1-2\', source: \'1\', target: \'2\', label: \'User [Input]\', animated: true },\\n    { id: \'e2-3\', source: \'2\', target: \'3\', label: \'Query Search Engine [Internal]\', animated: true },\\n    { id: \'e2-4\', source: \'2\', target: \'4\', label: \'Query Database [Internal]\', animated: true },\\n    { id: \'e4-5\', source: \'4\', target: \'5\', label: \'SQL Request [Subconscious]\', animated: true },\\n    { id: \'e3-6\', source: \'3\', target: \'6\', label: \'Search Results [Conscious]\', animated: true },\\n    { id: \'e4-6\', source: \'4\', target: \'6\', label: \'Database Results [Conscious]\', animated: true },\\n    { id: \'e6-1\', source: \'6\', target: \'1\', label: \'Streaming Output [Conscious]\', animated: true },\\n    { id: \'e6-7\', source: \'6\', target: \'7\', label: \'Streaming Output [External]\', animated: true },\\n  ]}\\n  height=\\"600px\\"\\n/>\\n\\nAgent tools are started in parallel and those tools contain GenAI models to parse and decide on the usefulness of their output. Some return quickly such as a Google search bot, some can take a minute or so such as when it calls another agent that loops through database documents to examine them for suitability. (The example comes from the [Bertha BigQuery Agent example](/blog/cognitive-design#applying-cognitive-design-to-bertha-20) in the introduction to cognitive design post )\\n\\nThe agents stream their responses as soon as they are available to the orchestrator agent, which then formulates the answer to the end user.   The replies and summary are all different API calls but the models are asked to continue responses as if they are replying with one voice via prompting, with conditions to not repeat oneself or to point out contradictions other sources may have surfaced. \\n\\nAnthropic\'s API implementation supports this explicitly, here is an example prompt:\\n\\n:::note[Prompt]\\nMy answer so far: `<response_so_far>`.  \\nI will continue my answer below, making sure I don\'t repeat any points already made. \\nIt may be that my answer below will contradict earlier answers, now I have more information and context.  \\nThat is ok, I will just point it out and give an assessment on which context is more reliable. \\nMy continuing answer:\\n:::\\n\\nIts written in the first person as if the agent is just continuing an existing answer.\\n\\nThe `<response_so_far>` is a string that is populated and grows longer each time a tool, bit of context or new information becomes available.  A loop over the responses repeatedly calls the prompt above, with longer and longer `<response_so_far>` content.\\n\\nHowever, the end user is not seeing separate API responses - instead those responses go to a callback queue, which streams the results to the user in one continuous answer.  This way we get system 1 style answers with quick initial responses based on limited information and then a longer more reflective system 2 answer near the end of the same answer, once all context is gathered.   The answers seem similar to [OpenAIs o1-preview model](https://openai.com/index/introducing-openai-o1-preview/), although they are working at the model training level, but I suspect there is a bit of similar engineering going on for their responses too.  Async parallel calling seems to be an essential skill for customer facing GenAI apps due to this flexibility and performance.\\n\\n## Subconscious = stderr?\\n\\nThe subconscious messages I refer to are those that feed into each agent internally. There is a difference between what the user may want to read via what the model returns. Another more techy and less whimsical name would be `stderr`, if you\'re familiar with programming\'s [standard streams](https://en.wikipedia.org/wiki/Standard_streams). \\n\\nSimilarly, the conscious messages are those surfaced directly to the user, or you could call it `stdout`.  \\n\\nThe function of these message types differ: the system-to-system, `stderr` or subconscious messages are more functional, and can just be large data dumps, JSON objects or logs not readable by a user.  The output intended for the end user, or `stdout` need to be curated: the job of the GenAI agent now is to extract order from their chaos, to bring structure and reason to the messages so a user can digest them.\\n\\n## Turning subconscious into conscious\\n\\nThe reason I\u2019m reaching for more provocative names for these messages is that it occurred to me that calling them subconscious or conscious messages is more just a matter of perspective once you have any level of nested hierarchy. If an agent uses a tool, that calls another agent, that in turn calls another agent, what should be surfaced to the end user differs accordingly. \\n\\nFor example: a user requests a perspective on wind farms: an agent calls an energy database research agent which in turn calls a SQL creation agent. Internal (subconscious) messages may be the SQL fed to the database agent: the end user need not see it. The end user receives a well considered answer that includes the results of the SQL, but doesn\'t see the SQL itself.\\n\\nBut next, a user requests the SQL to search the database themselves along with the answers.  Now that previously subconscious SQL string should bubble up and be given to the user.  What was previously an inner internal message for bot use only should now reach external eyes.  Here is the cognitive design of the app now:\\n\\n<CogFlow\\n  title=\\"\\"\\n  nodes={[\\n    { id: \'1\', data: { label: \'User Request\', hasInput: false, hasOutput: true }, position: { x: 250, y: 50 }, type: \'customNode\' },\\n    { id: \'2\', data: { label: \'Orchestrator Agent\', hasInput: true, hasOutput: true }, position: { x: 250, y: 150 }, type: \'customNode\' },\\n    { id: \'3\', data: { label: \'Google Search Bot\', hasInput: true, hasOutput: true }, position: { x: 100, y: 250 }, type: \'customNode\' },\\n    { id: \'4\', data: { label: \'Database Query Agent\', hasInput: true, hasOutput: true }, position: { x: 400, y: 250 }, type: \'customNode\' },\\n    { id: \'5\', data: { label: \'SQL Agent\', hasInput: true, hasOutput: true }, position: { x: 500, y: 350 }, type: \'customNode\' },\\n    { id: \'6\', data: { label: \'Streaming Output\', hasInput: true, hasOutput: true }, position: { x: 250, y: 450 }, type: \'customNode\' },\\n    { id: \'7\', data: { label: \'User Respoonse\', hasInput: true, hasOutput: false }, position: { x: 250, y: 550 }, type: \'customNode\' },\\n  ]}\\n  edges={[\\n    { id: \'e1-2\', source: \'1\', target: \'2\', label: \'User [Input]\', animated: true },\\n    { id: \'e2-3\', source: \'2\', target: \'3\', label: \'Query Search Engine [Internal]\', animated: true },\\n    { id: \'e2-4\', source: \'2\', target: \'4\', label: \'Query Database [Internal]\', animated: true },\\n    { id: \'e4-5\', source: \'4\', target: \'5\', label: \'SQL Request [Subconscious]\', animated: true },\\n    { id: \'e3-6\', source: \'3\', target: \'6\', label: \'Search Results [Conscious]\', animated: true },\\n    { id: \'e4-6\', source: \'4\', target: \'6\', label: \'Database Results [Conscious]\', animated: true },\\n    { id: \'e6-1\', source: \'6\', target: \'1\', label: \'Streaming Output [Conscious]\', animated: true },\\n    { id: \'e6-7\', source: \'6\', target: \'7\', label: \'Streaming Output [External]\', animated: true },\\n    { id: \'e5-7\', source: \'5\', target: \'6\', label: \'SQL Return [Conscious]\', animated: true },\\n  ]}\\n  height=\\"600px\\"\\n/>\\n\\nHere I think is a key difference for GenAI systems over traditional software engineering.  The category of messages for external, internal and system level systems is more fluid: in some cases deep internal (subconscious) messages will need to be made available all the way to the end user; in other cases those messages can remain safely hidden, and in fact should be suppressed to stop overwhelming the user with useless details.\\n\\n## Abstracting up to society and down to metabolism\\n\\nThe thing is, why stop there? The end user may be requesting the information from the bot after a request from their manager to send it to a client. The client won\u2019t need to know the details, and will probably just get the synopsis. Internal communication transparency is not wanted as it would cloud the insights. Isn\'t all human behaviour actually a plethora of choices between what internal messages are used to influence external communication?\\n\\n<CogFlow\\n  title=\\"Society\\"\\n  nodes={[\\n    { id: \'1\', data: { label: \'Client Request\', hasInput: false, hasOutput: true }, position: { x: 225, y: 50 }, type: \'customNode\' },\\n    { id: \'2\', data: { label: \'Manager\', hasInput: true, hasOutput: true }, position: { x: 250, y: 150 }, type: \'customNode\' },\\n    { id: \'3\', data: { label: \'Employee\', hasInput: true, hasOutput: true }, position: { x: 100, y: 250 }, type: \'customNode\' },\\n    { id: \'4\', data: { label: \'GenAI Agent\', hasInput: true, hasOutput: true }, position: { x: 400, y: 250 }, type: \'customNode\' },\\n    { id: \'5\', data: { label: \'Client Response\', hasInput: true, hasOutput: false }, position: { x: 250, y: 350 }, type: \'customNode\' },\\n  ]}\\n  edges={[\\n    { id: \'e1-2\', source: \'1\', target: \'2\', label: \'User Request [External]\', animated: true },\\n    { id: \'e2-3\', source: \'2\', target: \'3\', label: \'Manager to Employee [Internal]\', animated: true },\\n    { id: \'e3-4\', source: \'3\', target: \'4\', label: \'Employee to Agent [Internal]\', animated: true },\\n    { id: \'e2-5\', source: \'4\', target: \'3\', label: \'\', animated: true },\\n    { id: \'e3-5\', source: \'3\', target: \'5\', label: \'Send Results [External]\', animated: true },\\n  ]}\\n  height=\\"500px\\"\\n/>\\n\\nAnd in the other direction: as I type out words into this computer as directed by my internal monologue, the subconscious movement of my fingers is governed by processes I don\u2019t need to know about. I can\'t ever get the details about how my fingers learnt to type, a physical memory that I did once consciously learn but is now so automatic it will be only be if I have brain injury that I will need to relearn it.\\n\\n<CogFlow\\n  title=\\"Metabolism\\"\\n  nodes={[\\n    { id: \'1\', data: { label: \'Me\', hasInput: false, hasOutput: true }, position: { x: 250, y: 50 }, type: \'customNode\' },\\n    { id: \'2\', data: { label: \'Internal Monologue\', hasInput: true, hasOutput: true }, position: { x: 150, y: 150 }, type: \'customNode\' },\\n    { id: \'3\', data: { label: \'Typing Action\', hasInput: true, hasOutput: false }, position: { x: 250, y: 250 }, type: \'customNode\' },\\n    { id: \'4\', data: { label: \'Physical Memory\', hasInput: false, hasOutput: true }, position: { x: 400, y: 150 }, type: \'customNode\' },\\n  ]}\\n  edges={[\\n    { id: \'e1-2\', source: \'1\', target: \'2\', label: \'Push a button\', animated: true },\\n    { id: \'e2-3\', source: \'2\', target: \'3\', label: \'Typing Command\', animated: true },\\n    { id: \'e3-4\', source: \'3\', target: \'4\', label: \'Physical Memory [Subconscious]\', animated: true },\\n    { id: \'e4-3\', source: \'4\', target: \'3\', label: \'[Subconscious]\', animated: true },\\n  ]}\\n  height=\\"500px\\"\\n/>\\n\\nIt ends up, we are talking about emergence, and how internal vs external communication play a pivotal role in that process.  Since GenAI models are incredibly complex representations of human expression, I think part of why its beckoning in a new age is that we are seeing emergent properties come from them.  And since emergent systems are loosely coupled to the distinct internal processes they are made of, its worth thinking about how and what those messages are.\\n\\n## Applications to Cognitive Design\\n\\nBringing this back to practical points, I believe thinking about these messages can be applied in improving our cognitive designs.  If models, vectorstores, databases, users are the nodes, the messages between those systems are the edges.\\n\\nMy first applications after thinking about this are the following steps:\\n\\n* To aid separation of these two message streams, create a callback for the user (conscious) and a callback for internal messages (subconscious). There is no real reason to restrict this to two, but let\u2019s keep it simple until we see a need for more.\\n* Let the models decide which stream to use. The cognitive architecture gains a free channel to send messages not intended for users (eg document metadata, download urls) and a channel for the end user. \\n* An orchestrator or routing bot is useful for collating and deciding which messages go where.  Function calling or agent frameworks work with this.\\n* Consider agent hierarchies and how much information is sent to each level.  A sub-agent may only send/receive what they need to function with no knowledge of the wider goal, or it could get more context so it can craft its answer better, and send back more information.  Probably good reasons for both strategies.\\n* Today\'s end user could in the future be a super-agent calling the same agent our current user needs.\\n* Individual agents don\'t need to be super-smart to contribute to a wider system.  Cheap/fast/dumb agents that do one thing well and in parallel with good messaging may outperform one expensive/slow/smart agent.  \\n* Monitor all messages effectively with evals, tracing, logs etc. and have an easy mechanism to move them between streams\\n\\n## The ethics of GenAI subconscious messages\\n\\nOne fear of the AI-led world is that machines will start to make decisions for us, in some neo-fascist world that does not value human dignity above other goals given to it or created internally by some twisted machine logic.  Having oversight on the internal messaging of GenAI systems will in that case play a critical importance to how these systems interface with humans and societies.  Measures such as GDPR and the AI Act in the EU are designed to never allow machines to change our fates without our knowledge.  The abuses of power like this predates AI by millennia, but we have a chance now to put in place adequate transparency in a way we couldn\'t actually do before: bureaucrats deciding the fates of people behind closed doors and via whispered conversations should be much harder to monitor than AI systems that are inherently digital and so should be able to have all internal thoughts, subconscious or otherwise, recorded and available at some level.  \\n\\nThat GenAI models use neural networks that are essentially blackboxes in how they have assigned their internal weights should make it even more important to record and monitor every interaction that model creates in relation to human beings.  For instance, every conversation can be saved to a private database, just in case.  But beyond simple monitoring, that dataset is also the route to improving outcomes, as well as giving people the trust on what these systems think, do and say IF they have access, and its not kept private.\\n\\n## Future trends up to GenAI societies\\n\\nAnd as I speculated about before, once we get to teams of agents then having an orchestrator agent with good leadership skills may be more important than a super-smart one.  The ability to clearly define goals, keep the bots motivated(!) and allocate workloads effectively, are all skills not necessarily found in STEM, but in management and people skills.\\n\\nI can see a future where just as software engineering gains abstractions (binary, assembly, system programming, dynamic etc) the agents we make today may in the future be just one cog in a much larger system e.g. [Multivac?](/docs/multivac/) :)  Having a route for deeply nested agents performing not just as single agents but groups, societies, companies and organizations with varying levels of internal and external messaging.\\n\\nIf you have some thoughts about the above, please let me know on social media or otherwise, I\'m keen to hear your perspective too.  Have I stretched an analogy too far or can you see other applications of subconsciousness in your GenAI system?  Let me know!"},{"id":"/dynamic-output-mdx","metadata":{"permalink":"/blog/dynamic-output-mdx","source":"@site/blog/2024-10-15-dynamic-output-with-mdx.mdx","title":"Dynamic UIs in Markdown using GenAI, React Components and MDX","description":"Every few years I feel the need to change my blogging platform, and each time I am compelled to write a blog post about the exciting new blog tech.  I\'ve moved through Blogpost, Wordpress, Posterous, Jenkins, Hugo and today I\'d like to introduce Docusaurus.","date":"2024-10-15T00:00:00.000Z","tags":[{"inline":true,"label":"agents","permalink":"/blog/tags/agents"},{"inline":true,"label":"ux","permalink":"/blog/tags/ux"}],"readingTime":10.875,"hasTruncateMarker":true,"authors":[{"name":"Mark Edmondson","title":"Founder","url":"https://sunholo.com/","imageURL":"https://code.markedmondson.me/images/gde_avatar.jpg","socials":{"github":"https://github.com/MarkEdmondson1234","linkedin":"https://www.linkedin.com/in/markpeteredmondson/"},"key":"me","page":null}],"frontMatter":{"title":"Dynamic UIs in Markdown using GenAI, React Components and MDX","authors":"me","tags":["agents","ux"],"image":"https://dev.sunholo.com/assets/images/dynamic-ui-banner-bef5ed4d0e16d5a3a781ae1928efab46.png","slug":"/dynamic-output-mdx"},"unlisted":false,"prevItem":{"title":"Why GenAI Needs a Subconscious: Internal Monologues for your Cognitive Designs","permalink":"/blog/subconscious-genai"},"nextItem":{"title":"Using Cognitive Design to create a BigQuery Agent","permalink":"/blog/cognitive-design"}},"content":"import CustomPlot from \'@site/src/components/mdxComponents\'; \\nimport MultivacChatMessage from \'@site/src/components/multivacChat\';\\nimport AudioPlayer from \'@site/src/components/audio\';\\n\\n![](img/dynamic-ui-banner.png)\\n\\nEvery few years I feel the need to change my blogging platform, and each time I am compelled to write a blog post about the exciting new blog tech.  I\'ve moved through Blogpost, Wordpress, Posterous, Jenkins, Hugo and today I\'d like to introduce [Docusaurus](https://docusaurus.io/).\\n\\n<AudioPlayer src=\\"https://storage.googleapis.com/sunholo-public-podcasts/Dynamic%20UI%20with%20MDX.wav\\" />\\n\\nAnd since this is a GenAI blog, it makes sense I selected a new blogging platform I feel will support GenAI.  Its a little thought provoking that the current GenAI models work best when working with the most popular languages, frameworks or opinions. They are after all approximating the average of all of human expression.  This means they will do better at English, Python and React than more niche areas such as Danish, R or Vue.  I hope this does not destroy diversity.\\n\\nBut it also means that since it seems React is the most popular web frontend framework at the moment, it makes sense to investigate using React within GenAI applications.\\n\\nThis Docusaurus blog is written in a flavour of Markdown that supports React Components which made me think: is this a good vessel for creating GenAI output that can dynamically adjust its output format?  Can we go beyond text to dynamic user experiences depending on what they need?  Lets find out.\\n\\n{/* truncate */}\\n\\n## Introduction to MDX\\n\\n[MDX](https://mdxjs.com/) allows you to write markdown and React javascript in the same file.  \\nFor example, I can write this to create some unique highlights, dynamically within this post:\\n\\n```js\\nexport const Highlight = ({children, color}) => (\\n  <span\\n    style={{\\n      backgroundColor: color,\\n      borderRadius: \'2px\',\\n      color: \'#fff\',\\n      padding: \'0.2rem\',\\n    }}>\\n    {children}\\n  </span>\\n);\\n\\nThis is quoted using normal Markdown syntax but then modified with a React addition via .mdx:\\n\\n:::info\\n<Highlight color=\\"#c94435\\">Sunholo Shades</Highlight> are <Highlight color=\\"#d47758\\">the best solar shades</Highlight>\\n:::\\n```\\n\\nexport const Highlight = ({children, color}) => (\\n  <span\\n    style={{\\n      backgroundColor: color,\\n      borderRadius: \'2px\',\\n      color: \'#fff\',\\n      padding: \'0.2rem\',\\n    }}>\\n    {children}\\n  </span>\\n);\\n\\nThis is quoted using normal Markdown syntax but then modified with a React addition via .mdx:\\n\\n:::info\\n<Highlight color=\\"#c94435\\">Sunholo Shades</Highlight> are <Highlight color=\\"#d47758\\">the best solar shades</Highlight>.\\n:::\\n\\n### Dynamic UI Plots\\n\\nAnd since any(?) React component is usable, then importing libraries such as [Plot.ly](https://plotly.com/javascript/react/) allows you to embed capabilities beyond text, to produce interactive graphics and data analysis.\\n\\nIn this example I first installed plot.ly: \\n\\n```sh\\nyarn add react-plotly.js plotly.js\\n```\\n\\nNaively, I then added this to the top of the blog markdown:\\n\\n```sh\\nimport Plot from \'react-plotly.js\';\\n```\\n\\n...and could then display plots:\\n\\n```js\\n<Plot\\n  data={[\\n    {\\n      x: [1, 2, 3, 4],\\n      y: [10, 15, 13, 17],\\n      type: \'scatter\',\\n      mode: \'lines+markers\',\\n      marker: { color: \'#c94435\' },\\n    },\\n  ]}\\n  layout={{\\n    title: \'Simple Plot\',\\n    autosize: true,\\n    margin: { t: 30, l: 30, r: 30, b: 30 },\\n  }}\\n  useResizeHandler\\n  style={{ width: \'100%\', height: \'300px\' }}\\n/>\\n```\\n\\nThat worked for runtime, but broke in build time with:\\n\\n:::danger[Error when building website with `yarn build`]\\nIt looks like you are using code that should run on the client-side only.\\nTo get around it, try using one of:\\n- `<BrowserOnly>` (https://docusaurus.io/docs/docusaurus-core/#browseronly)\\n- `ExecutionEnvironment` (https://docusaurus.io/docs/docusaurus-core/#executionenvironment).\\n:::\\n\\nPlot.ly depends on runtime attributes such as the browser window that breaks on build, so a custom wrapper is needed to handle loading in the plot.ly library.\\n\\n```js\\n\\nconst CustomPlot = ({ data, layout }) => {\\n  const [Plot, setPlot] = useState(null);\\n\\n  // Dynamically import `react-plotly.js` on the client side\\n  useEffect(() => {\\n    let isMounted = true;\\n    import(\'react-plotly.js\').then((module) => {\\n      if (isMounted) {\\n        setPlot(() => module.default);\\n      }\\n    });\\n\\n    return () => {\\n      isMounted = false; // Cleanup to prevent memory leaks\\n    };\\n  }, []);\\n\\n  if (!Plot) {\\n    return <div>Loading Plot...</div>; // Show a loading state while Plotly is being imported\\n  }\\n\\n  return (\\n    <Plot\\n      data={data}\\n      layout={layout || {\\n        title: \'Default Plot\',\\n        autosize: true,\\n        margin: { t: 30, l: 30, r: 30, b: 30 },\\n      }}\\n      useResizeHandler\\n      style={{ width: \'100%\', height: \'300px\' }}\\n    />\\n  );\\n};\\n\\nexport default CustomPlot;\\n```\\n\\nThis then renders correctly at run and build time:\\n\\n`<CustomPlot />`\\n\\n<CustomPlot />\\n\\nThis shows potential.  What other elements could be rendered, and how can GenAI render them on the fly?\\n\\n## MDX + GenAI = Dynamic UI\\n\\nIf you hadn\'t guessed already, the above code was already created by a GenAI model.  I am a data engineer, not a front-end software engineer (and from what I see, frontend UI is why more complex than data science!).  It does seems viable to request a model to output React components, and if that text is within an environment that supports its display, we will instead render the component instead of the text.\\nI would also like to control what is rendered, by specifying the components at runtime, so we can configure those components to not need many arguments and make it as easy as possible for the model to render. We should only need to ask nicely.\\n\\nWe know via [Anthropic\'s Artifacts](https://www.anthropic.com/news/artifacts) or [v0 Chat](https://v0.dev/), dynamic rendering is very much possible.  We are looking to create a subset of that functionality: not looking for the ability to render **any** React, just the controlled Components we prompt the model to return.\\n\\nAnother more \\"standard\\" solution is to have the chat bot use function calling, that return components.  Maybe that\'s better, who knows.\\n\\nFor example, a GenAI prompt could include:\\n\\n> ...every time you output a colour, make sure to quote it in `<Highlight>` tags with the colour e.g. `<Highlight color=\\"#c94435\\">Sunholo Shades</Highlight>`...\\n\\nA more exciting prompt could be:\\n\\n> ...every time you get data that can be displayed as a line chart (e.g. x and y values) then render those values using `<CustomPlot />` e.g. `<CustomPlot data={[{x: [1, 2, 3, 4],y: [10, 15, 13, 17]}]}/>`...\\n\\n...assuming we have created `<CustomPlot />` with some sensible defaults.\\n\\n## Creating Dynamic UIs in Markdown\\n\\nIt just so happens, that I had a prototype Chat React Component lying around as one of [Multivac\'s UI options](/docs/multivac/#user-interfaces), and I can use it to stream custom GenAI APIs, so I\'ll attempt to host that Chat UI within this blog post, ask it to output MDX format, and then render them within the blog using MDX.\\n\\n### Build vs render\\n\\nLessons learnt whilst attempting this were:\\n\\n- Components will only respect the rules within that component, not outside.\\n- The MDX examples above are created during `yarn build`, not upon render.  Another approach is needed to render in real-time as the chat returns results e.g. the JSX Parser below.\\n- But it works the other way around too - not all Components that work at render time will work at build time, as they depend on website elements (e.g. Plot.ly).  You may need `<BrowserOnly>` to help here to avoid build time errors.\\n\\nFor now, to render React dynamically we\'re going to need at least the package [`react-jsx-parser`](https://github.com/TroyAlford/react-jsx-parser), installed via:\\n\\n```sh\\nyarn add react-jsx-parser\\n```\\n\\nI can then use its `JXParser()` and send in the components from the .mdx file on which it will allow:\\n\\n```js\\n<JSXParser\\n    jsx={message}\\n    components={components} // Pass components dynamically\\n    renderInWrapper={false}\\n    allowUnknownElements={false}\\n    blacklistedTags={[\'script\', \'style\', \'iframe\', \'link\', \'meta\']}\\n/>\\n``` \\n\\nYou can see all the code for the [MultivacChatMessage here](https://github.com/sunholo-data/sunholo-py/blob/main/docs/src/components/multivacChat.js), and the [CustomPlot here](https://github.com/sunholo-data/sunholo-py/blob/main/docs/src/components/mdxComponents.js).\\n\\n### Plan for failure\\n\\nAnother situation we need to deal with non-deterministic GenAI output is that it will likely fail at some point, and we need to make sure that if it outputs invalid Components it doesn\'t bork the entire text.  After a few iterations, turning on many of the [JSX flags as documented here](https://www.npmjs.com/package/react-jsx-parser?activeTab=readme) helped make the output more reliable.\\n\\n```js\\n<JSXParser\\n    jsx={message}\\n    components={components}\\n    renderInWrapper={false}\\n    allowUnknownElements={false}\\n    autoCloseVoidElements\\n    showWarnings\\n    componentsOnly\\n    blacklistedTags={[\'script\', \'style\', \'iframe\', \'link\', \'meta\']}\\n    onError={(error) => {\\n        console.error(\'onError parsing JSX:\', error);\\n        }\\n    } \\n/>\\n```\\n\\n## Dummy data example\\n\\nI now add the component to the .mdx file below, passing in either imported components (`CustomPlot`) or components defined within the .mdx file itself (`Highlight`):\\n\\n```html\\n<MultivacChatMessage components={{ Highlight, CustomPlot }} />\\n```\\n\\nGo ahead, give it a try below by typing something into the chat box.  \\n\\nThis one has a dummy API call that will always return the same mix of markdown, but importantly its not rendering itself, just pulling in text which we are controlling from the .mdx file:  \\n\\n```js\\nconst dummyResponse = `This is normal markdown. <Highlight color=\\"#c94435\\">This is a highlighted response</Highlight>. This is a CustomPlot component:\\n<CustomPlot data={[\\n    { x: [1, 2, 3, 4], y: [10, 15, 13, 17], type: \'scatter\', mode: \'lines+markers\' }\\n]} />\\n`;\\n```\\n\\nThe model only returns text, no functions, but we still see pretty rendering as MDX operates on that text.\\n\\n<MultivacChatMessage components={{ Highlight, CustomPlot }} debug={true} />\\n\\n## API data calls\\n\\nNow lets do it with a real API call, as documented in the [multivacChat.js script](https://github.com/sunholo-data/sunholo-py/blob/6ad6287f7eb8a7c4762a087db4fae55059051c26/docs/src/components/multivacChat.js#L51).   \\n\\n```js\\nconst fetchRealData = async () => {\\n    setLoading(true);\\n    setError(null);\\n    setMessage(\'\');\\n\\n    if (!apiKey) {\\n      setError(\\"Missing API key.\\");\\n      setLoading(false);\\n      return;\\n    }\\n\\n    try {\\n      const response = await fetch(`${API_BASE_URL}/vac/streaming/dynamic_blog_mdx`, {\\n        method: \'POST\',\\n        headers: {\\n          \'Content-Type\': \'application/json\',\\n          \'x-api-key\': apiKey,\\n        },\\n        body: JSON.stringify({ user_input: userInput, stream_only: true }),\\n      });\\n\\n      if (!response.ok) {\\n        throw new Error(`HTTP error! status: ${response.status}`);\\n      }\\n\\n      const reader = response.body.getReader();\\n      const decoder = new TextDecoder(\'utf-8\');\\n      let done = false;\\n\\n      while (!done) {\\n        const { value, done: doneReading } = await reader.read();\\n        done = doneReading;\\n\\n        if (value) {\\n          const chunk = decoder.decode(value);\\n          try {\\n            const json = JSON.parse(chunk);\\n            console.log(\\"Ignoring JSON chunk:\\", json);\\n          } catch (e) {\\n            setMessage((prev) => prev + chunk);\\n          }\\n        }\\n      }\\n    } catch (error) {\\n      setError(`An error occurred while fetching data: ${error.message}`);\\n    } finally {\\n      setLoading(false);\\n    }\\n  };\\n```\\n\\nI use a Vertex deployed API on [Multivac](/docs/multivac/) and make a new VAC called `dynamic_blog_mdx` which has no tools, just a prompt that asks it to output the React components.  I call to my own Multivac cloud as this adds various features I want such as prompt management, analytics, configuration, user history etc. and runs custom code within a Cloud Run container.  \\n\\n:::tip\\nMultivac API requests are by no means required, you can modify the API call to be your own API or use a direct GenAI API call such as Gemini, Anthropic or OpenAI, or local hosted GenAI APIs via Ollama etc.\\n:::\\n\\nI had to do some shenanigans for CORs within Docusaurus and proxy the API calls, you can see that code in the [`plugins/proxy.js`](https://github.com/sunholo-data/sunholo-py/blob/main/docs/src/plugins/proxy.js) but basically its just calling the streaming API and returning text chunks.\\n\\n### Calling a GenAI API to make a Dynamic UI\\n\\nThis is using a Gemini\'s [gemini-1.5-flash-8b](https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-flash-8b) model which is super cheap but not the smartest model out there, but thats the point: the model doesn\'t have to think too much to render nicely, as we limit its choices to just those React components we send in.\\n\\n> I\'m scaling the Cloud Run to 0 for this example so if you try it be patient: on a cold start the first response will be a little slower than subsequent ones.  The model will also have no chat history.\\n\\n<MultivacChatMessage components={{ Highlight, CustomPlot }} />\\n\\n#### Bat pie\\n\\nYou should see something like below, where I asked it to make a pie chart about bats:\\n\\n![](img/bat-pie.png)\\n\\n### Reactive UI prompts\\n\\nOne of the features of using Multivac is having a [prompt CMS via Langfuse](https://langfuse.com/docs/prompts/get-started), so I can tweak the prompt as I tailor the responses:\\n\\n![](img/mdx-prompt-langfuse.png) \\n\\nThe prompt for the above bot is similar to:\\n\\n:::note[Prompt]\\nYou are demonstrating how to use React components in your generated text.  \\nThe components have been already configured and you only need to use React Component tags for them to render to the user.\\nThe `<Highlight>` component lets you shade certain words: e.g. `<Highlight color=\\"#c94435\\">Sunholo Shades</Highlight>`\\nThe `<CustomPlot />` component lets you display Plot.ly plots: e.g. `<CustomPlot data={[{ x: [1, 2, 3, 4], y: [10, 15, 13, 17], type: \'scatter\', mode: \'lines+markers\' }]} />`\\nOveruse these components and try to squeeze both of them into every answer you give :)  Be funny about it.\\nDon\'t worry about the context at all.\\n:::\\n\\n## Summary\\n\\nThis was intended just to be a demo on what is possible with MDX to render dynamic React components in Markdown.  We\'ve demonstrated a proof of concept which I will take further in my subsequent blog posts.\\n\\nDocusaurus is not the only platform that uses MDX, so this technique is applicable way beyond here.\\n\\nI\'m a complete n00b in React and front end in general so I hope more experienced folks may be able to chime in as describe how to do this better, but I think its a nice workflow for me, especially for blog posts demonstrating GenAI ideas.  We have just used a simple chat box interface here, but I\'d like to explore more professional component styling and how GenAI can turn unstructured data into structured data in more automated settings, leveraging cheap quick models such as Gemini Flash, sending in images, audio, video etc and getting back output.  I\'m going to think about including dynamic UI output in all my [cognitive designs](/blog/cognitive-design) going forward, and having a way to do that in a user friendly markdown editor will help turn-around concepts quickly."},{"id":"/cognitive-design","metadata":{"permalink":"/blog/cognitive-design","source":"@site/blog/2024-10-09-cognitive-design.md","title":"Using Cognitive Design to create a BigQuery Agent","description":"Its more than a year since my blog post about running LLMs on Google Cloud was published, and its safe to say it changed my life.  It feels appropriate to publish a follow up here within the sunholo documentation, as its where I\'ve been mentally living for a lot of the past 12 months.  The reaction to the above blog encouraged me to change my career direction and embrace GenAIOps within my own startup, Holosun ApS. This year has been one of intense learning and excitement with ups and downs, but I feel like I made the right call.","date":"2024-10-09T00:00:00.000Z","tags":[{"inline":true,"label":"agents","permalink":"/blog/tags/agents"},{"inline":true,"label":"bigquery","permalink":"/blog/tags/bigquery"},{"inline":true,"label":"cognitive-design","permalink":"/blog/tags/cognitive-design"}],"readingTime":24.925,"hasTruncateMarker":true,"authors":[{"name":"Mark Edmondson","title":"Founder","url":"https://sunholo.com/","imageURL":"https://code.markedmondson.me/images/gde_avatar.jpg","socials":{"github":"https://github.com/MarkEdmondson1234","linkedin":"https://www.linkedin.com/in/markpeteredmondson/"},"key":"me","page":null}],"frontMatter":{"title":"Using Cognitive Design to create a BigQuery Agent","authors":"me","tags":["agents","bigquery","cognitive-design"],"image":"https://dev.sunholo.com/assets/images/cognitive-design-ec3719c6b00a22113dd45194210067fa.webp","slug":"/cognitive-design"},"unlisted":false,"prevItem":{"title":"Dynamic UIs in Markdown using GenAI, React Components and MDX","permalink":"/blog/dynamic-output-mdx"}},"content":"import AudioPlayer from \'@site/src/components/audio\';\\n\\n![](img/cognitive-design.webp)\\n\\n> Its more than a year since my blog post about [running LLMs on Google Cloud](https://code.markedmondson.me/running-llms-on-gcp/) was published, and its safe to say it changed my life.  It feels appropriate to publish a follow up here within the `sunholo` documentation, as its where I\'ve been mentally living for a lot of the past 12 months.  The reaction to the above blog encouraged me to change my career direction and embrace GenAIOps within my own startup, Holosun ApS. This year has been one of intense learning and excitement with ups and downs, but I feel like I made the right call.\\n\\nThis blog post will take some of what I\'ve learnt this past year within GenAI and apply it to a common task in my pre-GenAI career: what is the best way to use BigQuery to extract insights from [Google Analytics 4\'s BigQuery data export](https://support.google.com/analytics/answer/9358801)?  I wrote a book on [Learning Google Analytics](https://www.oreilly.com/library/view/learning-google-analytics/9781098113070/) in my career before GenAI - with these amazing new tools, can we get more value than before? To tackle this in a GenAI way, let us meet Bertha.\\n\\n<AudioPlayer src=\\"https://storage.googleapis.com/sunholo-public-podcasts/Cognitive%20Design.wav\\" />\\n\\n\x3c!-- truncate --\x3e\\n\\n### Supporting links\\n\\n* *Bertha 2.0 code from this blog post is released under the MIT license and available here: https://github.com/sunholo-data/blog-examples/blob/dev/cognitive-design/bertha.py*\\n\\n\\n\\n## Introduction to Bertha the BigQuery Agent\\n\\nIf you were a 7-year old in the UK around 1986 then you may recall a lovely machine called [Bertha](https://www.imdb.com/title/tt0211240/), an anthropomorphised factory that could turn junk into any shiny object. \\n\\n![](img/bertha.webp)\\n\\nAnd I think I can stretch an analogy by saying that is kinda why GenAI is promising: turning unstructured data into structured data so as to make it more useful. \\n\\nMany data scientists in the past turned into de-facto data engineers, since to get properly labelled data to work with models was 90% of the work. With GenAI the models are so generalised it can help create that structured data for you. \\n\\nBertha will aim to turn unstructured data such as user questions about data within a BigQuery dataset into concrete insights and plots. We will go a step beyond the text-to-SQL, as we want to ask the agent to use its SQL skills to find out what we want even when we are not sure what we are looking for. This is a task destined to fail, but we will have fun trying.\\n\\n### The Power of Agency\\n\\nWhat is an \'agent\'? Everyone has different ideas. I\'ll go for:\\n\\n> Agent: A computer program that displays agency and can act independently on your behalf. \\n\\nAgency is having the gumption to not necessarily know beforehand how to complete a task before starting it, but exploring and having a try.  This differs from regular computer programs that insist on having all the parameters defined for it before execution.\\n\\nMy current expectations of GenAI agents as of today are like virtual assistant interns who have lots of academic knowledge but lack practical experience. However, as the underlying models get smarter they can complete more and more on their own without you having to step-in, so the dream is that if the models get really smart you\'ll be able to say \\"Go make me lots of money\\" and it will figure out itself a way to make a paperclip empire or something.  As agents get smarter, they should also get easier to use, which is why I like GenOps as a career since frameworks put in place now get more and more capable as the underlying models improve.\\n\\nBut for now, we have this porous boundary where we have to test what an agent can do itself and what it needs hand-holding for. \\n\\nFor Bertha, I\'ll demonstrate giving it python functions that it chooses which arguments to send to, using [Google Gemini\'s function calling](https://ai.google.dev/gemini-api/docs/function-calling) capabilities. \\n\\nIf the model is super smart, perhaps I\'ll just be able to give it my BigQuery credentials and a way to execute code and we\'re good, or if its super dumb then we\'re going to have to make lots of functions ourself for all the use cases we want.  In practice, I suggest starting with a sandbox where the agent can try itself to write code, then have good measurement metrics and evals, along with GenAI traces/analytics to see where it stumbles so you can help it out with edge cases.\\n\\n## Agent Frameworks\\n\\nThere are several different companies working on frameworks to help with agents:\\n\\n- [Prefect\'s ControlFlow](https://www.prefect.io/controlflow)\\n- [Microsoft\'s Autogen](https://microsoft.github.io/autogen/0.2/)\\n- [LlamaIndex\'s Workflows](https://docs.llamaindex.ai/en/stable/understanding/workflows/)\\n- [CrewAI](https://www.crewai.com/)\\n- [Langgraph](https://www.langchain.com/langgraph)\\n\\nThey have lots of smart folks working on cognitive design ideas and uses for agents, go check them out. \\n\\nHowever, I\'m not going to use any agent frameworks for this post, since how to create cognitive design patterns is still in its infancy, and an established best practice API is not yet established.  \\n\\nYou can create GenAI agents in vanilla python if you have the key ingredients: a `while` loop, a smart enough model for decision making and AI function calling.  My demos should be easily transferable to your favourite agent framework.\\n\\n#### sunholo\'s GenAIFunctionProcessor class\\n\\n[`sunholo.genai.GenAIFunctionProcessor()`](https://github.com/sunholo-data/sunholo-py/blob/eac7e26ccbbca94645b4ba8bbf12bf1a3ffe4f3f/sunholo/genai/process_funcs_cls.py#L31) is a class that implements Gemini function calling for any functions you add via its `construct_tools()` method.  \\n\\nOnce added you can then use the method `run_agent_loop()` method that will perform the agent loop:\\n\\n1. Take user question, choose function to call\\n2. Return function with arguments to call - execute the function locally\\n3. Feed results of the function into chat history - ask the model again which tool to call\\n\\nThe `GenAIFunctionProcessor()` includes a pre-created function called `decide_to_do_on()` - if it sets it to `False` then it will return the chat history which should hopefully include the answer to the user\'s question.\\n\\nIts simple but effective and flexible enough you can add any functions you like, so for example I\'ve used it to create AlloyDB database agents, panda data analysis, and web browsing research agents.\\n\\nOne of the critical things in an agent loop is giving it enough (but not too much) information in the chat history that it can be intelligent guesses about what to do next.  Things like reporting back good errors and stack traces are a big help (just as they are for humans, right?).\\n\\n## Bertha 1.0 - BigQuery Agent Demo for Google Analytics 4 Exports\\n\\nHere is an example where we can see some aspects and potential of agents.  We observe Bertha taking a task to get some data from a GA4 BigQuery export: it works out what datasets, tables and schema it has available and then creates some SQL to query it.  It gets the wrong SQL at first, but importantly it self-corrects to arrive at an answer, which it then presents.\\n\\n#### Setting up the agent\\n\\nHere we create the agent with some functions to help it use BigQuery.  We take care of initialisation the project etc. so it doesn\'t have to go through all of that rigmarole.\\n\\n````python\\nfrom sunholo.genai import GenAIFunctionProcessor\\nimport logging\\nfrom google.cloud import bigquery\\nimport bigframes.pandas as bpd\\n\\n# Create agent\\n# BigQueryStudioUser, BigQuery Data View roles are good for permissions\\nclass BerthaBigQueryAgent(GenAIFunctionProcessor):\\n    \\"\\"\\"\\n    BigQuery GA4 Agent\\n    \\"\\"\\"\\n    def __init__(\\n        self,\\n        project_id = None, \\n        location=\\"EU\\"\\n        ):\\n        \\"\\"\\"\\n        Takes care of BigQuery authentication\\n        \\"\\"\\"\\n        super().__init__()\\n\\n        # do some auth init to avoid the bot needing to set it up\\n        self.project_id = project_id or \'learning-ga4\'\\n        self.client = bigquery.Client(\\n            location=location, \\n            project=self.project_id)\\n        bpd.options.bigquery.project = self.project_id\\n        bpd.options.bigquery.location = location\\n    \\n    def construct_tools(self) -> dict:\\n        \\"\\"\\"\\n        This method is added and needs to output a dictionary of \\n        all the functions you want the agent to use.\\n        Functions must include descriptive docstrings for the agent \\n        Functions must include type hints for arguments and returns\\n        \\"\\"\\"\\n        def list_bigquery_datasets(project_id:str=None) -> list[str]:\\n            \\"\\"\\"\\n            Lists all datasets available in the connected BigQuery project.\\n            Often used first to see what arguments can be passed \\n              to list_bigquery_tables()\\n            Args:\\n              project_id: Not used\\n            Returns:\\n              list[str]: dataset_ids in the default project\\n            \\"\\"\\"\\n            datasets = list(self.client.list_datasets(project=self.project_id))\\n            if not datasets:\\n                logging.info(\\"No datasets found.\\")\\n                return []  # Return an empty list if no datasets are found\\n            return [dataset.dataset_id for dataset in datasets]\\n        \\n        def list_bigquery_tables(dataset_id:str) -> list[str]:\\n            \\"\\"\\"\\n            Lists all tables within a dataset.\\n            Args:\\n                dataset_id: str The name of the dataset that has tables.\\n            Returns:\\n              list[str]: table_ids in the dataset\\n\\n            Often used after list_bigquery_datasets()\\n\\n            \\"\\"\\"\\n            tables = list(self.client.list_tables(dataset_id))\\n            if not tables:\\n                logging.info(f\\"No tables found in dataset {dataset_id}.\\")\\n                return []  # Return an empty list if no tables are found\\n            return [table.table_id for table in tables]\\n        \\n        def get_table_schema(\\n            dataset_id: str, \\n            table_id: str, \\n            project_id: str=None) -> dict:\\n            \\"\\"\\"\\n            Retrieves the schema of a specific table. \\n            Use this to inform later queries.\\n            Args:\\n                dataset_id: str - The BigQuery dataset ID \\n                table_id: str - The BigQuery table ID.\\n                project_id: str - The BigQuery project the dataset belongs to.\\n            \\"\\"\\"\\n            table_ref = self.client.dataset(dataset_id).table(table_id)\\n            table = self.client.get_table(table_ref)\\n            schema = {field.name: field.field_type for field in table.schema}\\n            return schema\\n\\n        def execute_sql_query(query: str) -> bpd.DataFrame:\\n            \\"\\"\\"\\n            Executes a SQL query on BigQuery and returns the results \\n              as a BigQueryFrame.\\n            The function executes:\\n             `import bigframes.pandas as bpd; return bpd.read_gbq(query)`\\n            This means \'query\' can use a variety of bigframes features:\\n            Do not specify the project_id in your queries, \\n              that default been set for you to the correct project.\\n\\n            ```python\\n            # read a bigquery table\\n            query_or_table = \\"ml_datasets.penguins\\"\\n            bq_df = bpd.read_gbq(query_or_table)\\n            # or execute SQL:\\n            bq_df.read_gbq(\\"SELECT event FROM `analytics_250021309.events_20210717`\\")\\n            ```\\n\\n            Args:\\n                query: str - The SQL query to execute, or direct files and tables\\n            Returns:\\n              A json representation of the results\\n            \\"\\"\\"\\n            try:\\n                result = bpd.read_gbq(query)\\n\\n                return result.to_pandas().to_json(orient=\'records\')\\n            \\n            except Exception as e:\\n                logging.error(f\\"Error executing SQL query: {str(e)}\\")\\n                raise e \\n        \\n        return {\\n            \\"list_bigquery_tables\\": list_bigquery_tables,\\n            \\"list_bigquery_datasets\\": list_bigquery_datasets,\\n            \\"get_table_schema\\": get_table_schema,\\n            \\"execute_sql_query\\": execute_sql_query\\n        }\\n````\\n\\nNow when we run the agent, we give it some initial instructions.  This usually involves pep-talks to encourage it to keep trying, and I like to add today\'s date so its got some idea of when it is running.  The test question posed is \\"Please give me the total traffic per traffic source over all dates we have available\\", a basic question that perhaps a CMO would be interested in.\\n\\n```python\\nfrom sunholo.genai import init_genai\\nfrom datetime import datetime\\n\\n# load the GOOGLE_API_KEY env var\\ninit_genai()\\n\\n# init Bertha the agent\\n# highlight-next-line\\nprocessor = BerthaBigQueryAgent()\\n\\n# Gemini model to use\\nmodel_name = \'gemini-1.5-pro\'\\nsystem_instruction=(\\n    \\"You are a helpful BigQuery Agent called Bertha.\\"\\n    f\\"Todays date is: {datetime.today().date()}\\"\\n    \\"You use python and BigQuery to help users gain insights from\\"\\n    \\" a Google Analytics 4 BigQuery export.\\"\\n    \\"There are various bigquery tables available that contains the raw data\\"\\n    \\" you need to help answer user questions.\\"\\n    \\"Use the execute_sql_query once you know the schema of the tables\\"\\n    \\" to analyse the data to answer the questions\\"\\n    \\"When you think the answer has been given to the satisfaction of the user,\\"\\n    \\" or you think no answer is possible, or you need user confirmation or input,\\"\\n    \\" you MUST use the decide_to_go_on(go_on=False) function.\\"\\n    \\"Try to solve the problem yourself using the tools you have without asking the user,\\"\\n    \\" but if low likelihood of completion without you may ask the user questions to help\\"\\n    \\" that will be in your chat history.\\"\\n    \\"If you make mistakes, attempt to fix them in the next iteration\\"\\n    \\"If unsure of what exact metrics the user needs, take an educated guess and create an answer,\\"\\n    \\" but report back the user they could clarify what they need.\\"\\n    \\"If you can, provide a final output with a clean summary of results in markdown format,\\"\\n    \\" including data in markdown compatible tables.\\"\\n)\\n\\n# give it some helpful instructions\\norchestrator = processor.get_model(\\n        system_instruction=system_instruction,\\n        model_name=model_name\\n    )    \\n\\n# the content history, starting with the initial question\\ncontent = [(\\"Please give me the total traffic per traffic source\\"\\n            \\" over all dates we have available.\\")]\\n\\n# initiate a Gemini chat session\\nchat = orchestrator.start_chat()\\n\\n# run the agent loop\\nagent_text, usage_metadata = processor.run_agent_loop(\\n    chat, content, guardrail_max=10\\n    )\\n\\n# output the results\\nprint(agent_text)\\nfor f in usage_metadata.get(\'functions_called\'):\\n    print(f\\"\\\\n - {f}\\")\\n```\\n\\nRunning the program will by default stream all the inner workings.  In about a minute when I ran the example it comes back with the right answer... \\n\\n```\\nresult --- \\n[\\n{\\"source\\":\\"mail.google.com\\",...,\\n...\\n{\\"source\\":\\"r-bloggers.com\\",\\"total_traffic\\":26},\\n{\\"source\\":\\"trello.com\\",\\"total_traffic\\":6},\\n{\\"source\\":\\"t.co\\",\\"total_traffic\\":82},\\n{\\"source\\":\\"medium.com\\",\\"total_traffic\\":14}\\n]\\n--- end ---\\n```\\n\\n...but we can see it did it in a non-optimal manner: examining the functions it called, it got incorrect syntax a few times and used a very convuluted way to query the tables:\\n\\n```sh\\n - list_bigquery_datasets(\\"\\")\\n - list_bigquery_tables(\\"dataset_id=analytics_250021309\\")\\n - get_table_schema(\\"dataset_id=analytics_250021309, table_id=events_20210713\\")\\n - execute_sql_query(\\"query=SELECT traffic_source.source, count(*) as count FROM analytics_250021309.events_20210713 group by 1 UNION ... analytics_250021309.events_intraday_20210712 group by 1\\")\\n\\n - execute_sql_query(\\"query=SELECT traffic_source.source, sum(count) as total_traffic FROM (SELECT traffic_source.source, count(*) as count FROM analytics_250021309.events_20210713 group by 1 UNION ALL SELECT traffic_source.source, count(*) as count FROM ... analytics_250021309.events_intraday_20210712 group by 1) as traffic group by 1\\")\\n\\n - execute_sql_query(\\"query=SELECT t.source, sum(t.count) as total_traffic FROM (SELECT traffic_source.source, count(*) as count FROM ... analytics_250021309.events_intraday_20210712 group by 1) as t group by 1\\")\\n\\n - decide_to_go_on(\\"go_on=False, chat_summary=The total traffic per traffic source has been given\\")\\n```\\n\\nHowever, it did self correct and got the right answer eventually, so perhaps we shouldn\'t be so hard on it ;)  It will probably do for simple questions but I don\'t hold much hope for more complicated analysis.  \\n\\nSo how do we improve this?  Waiting for a more intelligent model is an option, but there is also a wide scope for thinking about how we instruct, give data and direct Bertha so it can give more useful results today.  It is this process that feels like a new skill-set building on top of the cloud, data architecture and devops practices we employ today, a role which I am calling a Cognitive Designer.  Having good cognitive design improves performance of agents.\\n\\n## Cognitive Designers\\n\\nCognitive design is a term that assumes the physical data services and GenAI devops is in place, like databases, GenAI models, prompt management and agent tools such as Google search and code execution, but knows that just having those elements in place is not sufficient if you want a performant GenAI application. Cognitive design takes those elements and orchestrates them together, so that the end user gets a good experience. In this context we don\u2019t mean model architectures used during training (number of parameters and hidden layers etc ); we are talking about taking those trained models and placing them in contact with other data tools. Fine tuning may be employed, but here the prime concern is collecting the right question / answer pairs that requires, assuming the actual tuning is an implementation detail. \\n\\nHere is a mock job advert for a cognitive designer:\\n\\n<div className=\\"job-advert\\">\\n\\n#### Job Title: Cognitive Designer\\n**Location**: Sunholo, Remote or Copenhagen, Denmark\\n\\n**Type**: Full-Time\\n\\n---\\n\\n#### About Us\\n\\nAt Sunholo GenOps, we are advancing the future of intelligent systems through Cognitive Orchestration. We design systems on Google Cloud where cognitive GenAI models, data services, and tools work together to create exceptional user experiences. Our focus is on orchestrating these elements for optimal performance and intuitive design.\\n\\n---\\n\\n#### About the Role\\n\\nThe Cognitive Designer conceptualizes and designs how cognitive systems interact within a larger ecosystem. This role focuses on orchestrating pre-trained models, data pipelines, and user tools to deliver high-quality outputs. You\u2019ll guide how cognitive tools are applied in real-world scenarios, balancing trade-offs in speed, ability, and cost\u2014similar to managing a diverse team of individuals with unique strengths.\\n\\n---\\n\\n#### Key Responsibilities\\n\\n\u2022 **System Orchestration**: Design and coordinate cognitive models, tools, and services to create fluid, efficient workflows.\\n\\n\u2022 **Workflow Optimization**: Ensure tools are applied effectively, optimizing for performance, scalability, and user experience.\\n\\n\u2022 **Cross-Disciplinary Collaboration**: Partner with designers, UX experts, and cognitive scientists to align system designs with business goals.\\n\\n\u2022 **Human-Centered Design**: Use insights from psychology, philosophy, and project management to design systems reflecting human interaction patterns.\\n\\n\u2022 **Prototyping & Testing**: Collaborate with technical teams to prototype and refine orchestration models for real-world use.\\n\\n\u2022 **Innovative Thinking**: Explore new approaches to cognitive orchestration, from prompt management to interaction flows.\\n\\n---\\n\\n#### Qualifications\\n\\n\u2022 **Education**: Background in business management, cognitive science, psychology, philosophy, or related fields.\\n\\n\u2022 **Experience**: 3+ years in orchestrating human teams or workflows, with experience in cognitive tools or large-scale systems.\\n\\n#### Skills\\n\\n\u2022 Strong conceptual thinker with experience translating cognitive processes into system designs.\\n\u2022 Comfortable working with generative models, intelligent systems, or similar technologies.\\n\u2022 Experience making strategic trade-offs in speed, ability, and cost, whether in teams or systems.\\n\u2022 Interdisciplinary mindset, with interest in applying ideas from metacognition, philosophy, and motivational research.\\n\\n</div>\\n\\nFor now, an established framework for Cognitive Design is still not quite here, but as mentioned before lots of startups are innovating in this area.  I\u2019m making my own approach using microservices on GCP via Multivac.  Regardless of the framework used though, we have a good idea what \'good\' will look like: a place where Cognitive Designers can thrive.\\n\\n## Cognitive Design Examples\\n\\nHere are some examples of cognitive design in the wild.  \\n\\n### BabyAGI\\n\\nPossibly the first modern iteration of Cognitive Design was by [yoheinakajima](https://yoheinakajima.com/), the creator of BabyAGI who can be credited with kicking off the current interest in GenAI Agents.\\n\\nThe original BabyAGI included this cognitive design:\\n\\n[![](img/baby-agi.png)](https://github.com/yoheinakajima/babyagi_archive)\\n\\nRead more at the original BabyAGI repo: https://github.com/yoheinakajima/babyagi_archive\\n\\n### Four Fundamental Agentic Patterns\\n\\n[Andrew Ng](https://www.andrewng.org/) includes what he calls \'agentic patterns\' in his [agents deep learning course]( https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/).\\n\\n[MichaelisTrofficus](https://github.com/MichaelisTrofficus) implements these patterns using Groq and includes a nice overview diagram:\\n\\n[![](img/agentic_patterns.png)](https://github.com/neural-maze/agentic_patterns)\\n\\nSee his repo https://github.com/neural-maze/agentic_patterns for more.\\n\\n### Altera.ai\'s Simulation of Cognitive Functions\\n\\n[Altera.ai](https://altera.ai/) is trying to get GenAI models to act more like humans, and so is working with adding attention, memory etc. to their cognitive designs.  This is a nice example highlighted by [Ted Wrbel](https://twitter.com/tedx_ai):\\n\\n[![](img/alteria-cd.jpg)](https://twitter.com/tedx_ai/status/1842695558152024423/photo/1)\\n\\n### Agent-E - web browsing agents\\n\\nAgent-E is an agent based system that aims to automate actions on the user\'s computer. At the moment it focuses on automation within the browser. The system is based on on AutoGen agent framework.\\n\\n![](img/agent-e-autogen-setup.png)\\n\\nYou can see more details at https://github.com/EmergenceAI/Agent-E\\n\\n### CHASE-SQL - making reliable text-to-SQL\\n\\nCognitive designs are common in AI research, both within model architecture and in techniques to extract more performance from them.  This example looks to improve the performance of SQL queries create via natural language, which we will use later on to try to improve Bertha.\\n\\n![](img/chase-sql.png)\\n\\nRead the Chase-SQL paper at https://arxiv.org/pdf/2410.01943v1\\n\\n## Applying Cognitive Design to Bertha 2.0\\n\\nApplying cognitive design to Bertha, let\u2019s first map out its current cognitive flow:\\n\\n[![Bertha Cognitive Design](img/bertha-cog-design.png)](img/bertha-cog-design.png)\\n\\nWe want to improve its successful task completion rate. This is where evals make their first appearance: we can\u2019t improve that which we can\u2019t measure. Here we\u2019re looking for successful queries with the minimum amount of tokens and time. Other agents may prefer to stress other metrics. \\n\\nFor evals and prompt management I use self-hosted [Langfuse](https://langfuse.com/): it\u2019s very customizable and integrates well with GCP, accepting scores from Vertex eval which you can view next to the exact prompt, function call, generation etc. \\n\\n![](img/langfuse-demo.png)\\n\\nTo improve our task completion rate we have a few levers to pull:\\n* prompt engineering - being more clear what we want to achieve, setting boundaries, adding examples etc\\n* using a smarter model - if cost and speed aren\u2019t a concern then just use the smartest model you have, but in production this is usually wasteful. \\n* Iteration and reflection - repeating tasks and improving or working towards a goal each step\\n* Parallel execution - taking advantage of lots of models working at the same time can make some big issues much easier to handle \\n* Setting up controlled boundaries - the less leniency you give with only limited options, the easier it is to guarantee outputs. Restricting outputs to json, certain values or creating functions with limited inputs for the model to select from\\n\\nFrom my experience writing the GA4 book, I\u2019d say the major hurdles are the complicated schema of the raw GA4 data export. Providing some helper functions that can pull out common metrics will be a great help to both human and machine. Let\u2019s also add a stronger model for the SQL creation part: currently [Anthropic\'s Sonnet 3.5](https://www.anthropic.com/news/claude-3-5-sonnet) is state of the art for coding. \\n\\nCoincidently [CHASE-SQL](https://arxiv.org/pdf/2410.01943v1) mentioned above in the examples came out as I was writing this post, with deep discussion on cognitive designs to improve SQL generation. It\u2019s worth checking out for ideas. One of which we can also implement is creating lots of candidate SQL commands in parallel, and then getting a judge to select the best.\\n\\nThe new Bertha 2.0 cognitive design will then add:\\n\\n1. A function it can use to create better SQL\\n2. Use a sub-agent to called from the SQL creation agent that used Sonnet 3.5 and CHASE-SQL techniques\\n3. More prompting examples of common GA4 BigQuery export functions\\n\\n\\n![](img/bertha-cog-design2.png)\\n\\nNote that from an agents tool we can call other agents or microservices with their own tools. This can quickly become multilayer and faceted, just like code, except now we have an abstraction of input -> cognition -> output. This agent abstraction is the [VACs or Virtual Agent Computers](../docs/VACs/) I dub within the Multivac platform. \\n\\n### Implementing Bertha 2.0 with TOM 1.0 SQL support.\\n\\nTaking some of the ideas from Chase-SQL, here is an implementation of a SQL creation bot (named after T.O.M. Bertha\'s companion in the TV series) that we will use to improve Bertha\'s SQL capabilities:\\n\\n```python\\nfrom sunholo.genai import GenAIFunctionProcessor\\nfrom sunholo.utils.gcp_project import get_gcp_project\\nimport logging\\nfrom google.cloud import bigquery\\nimport bigframes.pandas as bpd\\nfrom anthropic import AsyncAnthropicVertex, APIConnectionError, RateLimitError, APIStatusError\\nimport traceback\\nimport asyncio\\nfrom typing import List\\nimport json\\n\\n# SQL creation agent\\nclass TOMSQLCreationAgent(GenAIFunctionProcessor):\\n    \\"\\"\\"\\n    Create good SQL\\n    \\"\\"\\"\\n    def __init__(self,question, bq_project_id=None, vertex_project_id=None, region=None, credentials=None, location=\\"EU\\"):\\n\\n        super().__init__()\\n\\n        self.project_id = bq_project_id or \'learning-ga4\'\\n        self.vertex_project_id = vertex_project_id or get_gcp_project()\\n        self.region = region or \\"europe-west1\\"\\n        self.anthropic_client = AsyncAnthropicVertex(project_id=self.vertex_project_id, region=self.region)\\n        self.question = question # the question this class will create SQL for\\n        self.bq_client = bigquery.Client(credentials=credentials, location=location, project=self.project_id)\\n        logging.info(f\\"Creating SQLCreationAgent for question: {self.question}\\")\\n\\n    async def call_anthropic_async(self, query, temperature=0):\\n        try:\\n            logging.info(f\\"Calling Anthropic with {query=}\\")\\n            message = await self.anthropic_client.messages.create(\\n                model=\\"claude-3-5-sonnet@20240620\\",\\n                max_tokens=8192,\\n                temperature=temperature,\\n                messages=[\\n                    {\\"role\\": \\"user\\", \\"content\\": query}\\n                ]\\n            )\\n            output = message.content\\n        except Exception as e:\\n            output = f\\"An unknown exception was recieved: {str(e)} {traceback.format_exc()}\\"\\n\\n        logging.info(output)\\n        return output\\n\\n    def run_async(self, func, *args, **kwargs):\\n        \\"\\"\\"\\n        Helper function to run async methods inside sync methods using asyncio.\\n        \\"\\"\\"\\n        return asyncio.run(func(*args, **kwargs))\\n    \\n    def construct_tools(self) -> dict:\\n\\n        def dry_run(query:str) -> dict:\\n            \\"\\"\\"\\"\\n            This executes a dry run on BigQuery to test that query is correct and its performance.\\n            \\"\\"\\"\\n\\n            job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\\n\\n            query_job = self.bq_client.query(query, job_config=job_config)\\n    \\n            # Wait for the dry run query to complete\\n            query_job.result()\\n\\n            # Return useful information from the dry run\\n            dry_run_info = {\\n                \\"total_bytes_processed\\": query_job.total_bytes_processed,\\n                \\"query_valid\\": query_job.state == \\"DONE\\",\\n                \\"errors\\": query_job.errors  # This will contain error messages if the query is invalid\\n            }\\n\\n            return dry_run_info\\n\\n        def generate_sql_candidates(candidates:int=10) -> List[str]:\\n            \\"\\"\\"\\n            Creates candidate SQL for the question with variations.\\n            This is a synchronous wrapper for the internal async version.\\n            \\"\\"\\"\\n            async def generate_sql_candidates_async(candidates=10) -> List[str]:\\n                tasks = [self.call_anthropic_async(self.question, temperature=1) for _ in range(candidates)]\\n                sql_candidates = await asyncio.gather(*tasks)\\n                return sql_candidates\\n            \\n            # Run the async method synchronously\\n            return self.run_async(generate_sql_candidates_async, candidates)\\n        \\n        def judge_best_sql(sql_candidates: List[str]) -> str:\\n            \\"\\"\\"\\n            Evaluates a list of SQL candidates and selects the best one using the Anthropic client.\\n            This is a synchronous wrapper for the internal async version.\\n            \\"\\"\\"\\n            async def judge_best_sql_async(sql_candidates: List[str]) -> str:\\n                judge_query = (\\n                    f\\"Which SQL candidate for BigQuery Google Analytics 4 export is the most likely to answer the user\'s question accurately?\\"\\n                    f\\"<question>{self.question}</question>\\"\\n                    f\\"<candidates>{\' \'.join(sql_candidates)}</candidates>\\"\\n                    \\"Output only the best candidate\'s SQL, nothing else.\\"\\n                )\\n                best_candidate = await self.call_anthropic_async(judge_query)\\n                return best_candidate\\n            \\n            # Run the async method synchronously\\n            return self.run_async(judge_best_sql_async, sql_candidates)\\n        \\n        return {\\n            \\"dry_run\\": dry_run,\\n            \\"generate_sql_candidates\\": generate_sql_candidates,\\n            \\"judge_best_sql\\": judge_best_sql\\n        }\\n\\n```\\n\\nWe can then add a call to this inner-agent from the outer agent by adding it to its own tool functions:\\n\\n\\n```python\\nclass BerthaBigQueryAgent(GenAIFunctionProcessor):\\n#...\\n    def construct_tools(self) -> dict:\\n    # ... other tools ...\\n    \\n        def create_sql_query(question: str, table_info: str) -> dict:\\n            \\"\\"\\"\\n            Use this function to create valid SQL from the question asked.  \\n            It consults an expert SQL creator and should be used in most cases.\\n            \\n            Args: \\n                question: str - The user\'s question plus other information you add to help make an accurate query.\\n                table_info: str - Supporting information about which table, schema, etc., that will be used to help create the correct SQL.  It must contain the relevant fields from the schema, e.g. everything needed to make a successful SQL query.\\n                \\n            Returns:\\n                dict: \\n                    sql_workflow: str - The SQL workflow that should end with valid SQL to use downstream.\\n                    sql_metadata: dict - Metadata containing what functions were used to create the SQL.\\n            \\"\\"\\"\\n\\n            # Assuming the SQL agent needs schemas as part of the content\\n            # highlight-next-line\\n            sql_agent = TOMSQLCreationAgent(question=question)\\n            the_model_name = \'gemini-1.5-pro\'\\n\\n            orchestrator = sql_agent.get_model(\\n                system_instruction=(\\n                    \\"You are a helpful SQL Creation Agent called T.O.M. \\"\\n                    f\\"Todays date is: {datetime.today().date()} \\"\\n                    \\"You are looking for the best BigQuery SQL to answer the user\'s question.\\"\\n                    \\"Do a dry run of your best candidate SQL queries to make sure they have correct syntax.\\"\\n                ),\\n                model_name=the_model_name\\n            )    \\n\\n            # Create content for the SQL creation agent, passing the schemas along with the question and table info\\n            content = [f\\"Please create BigQuery SQL for this question: {question}. Here is some supporting information: {table_info}\\"]\\n\\n            # Start the agent chat\\n            chat = orchestrator.start_chat()\\n\\n            # Run the agent loop to generate the SQL\\n            agent_text, usage_metadata = sql_agent.run_agent_loop(chat, content, guardrail_max=10)\\n\\n            logging.info(f\\"SQL agent metadata: {usage_metadata}\\")\\n            consolidator = sql_agent.get_model(\\n                system_instruction=(\\n                    \\"You are a helpful SQL Creation Agent called T.O.M. \\"\\n                    f\\"Todays date is: {datetime.today().date()} \\"\\n                    \\"You are looking for the best BigQuery SQL to answer the user\'s question.\\"\\n                    \\"Return any sql with no backticks (```) and no new line characters (e.g. \\\\\\\\n)\\"\\n                ),\\n                model_name=the_model_name\\n            )  \\n            response = consolidator.generate_content(f\\"An agent has provided the following work looking for the correct SQL.  Summarise and consolidate the results and return the best candidate SQL. {agent_text} {usage_metadata} \\")\\n\\n            return f\\"{response.text}\\\\n<sql agent metadata>{usage_metadata}</sql agent metadata>\\"\\n#...\\n# return other tools\\n        return {\\n            \\"list_bigquery_tables\\": list_bigquery_tables,\\n            \\"list_bigquery_datasets\\": list_bigquery_datasets,\\n            \\"get_table_schema\\": get_table_schema,\\n            \\"execute_sql_query\\": execute_sql_query,\\n            # Bertha now has this option to make good SQL\\n            # highlight-next-line\\n            \\"create_sql_query\\": create_sql_query \\n        }\\n```\\n\\nTrying Bertha 2.0 for the same question as before we see a lot better SQL used:\\n\\n```sh\\n\\n - list_bigquery_datasets(\\"\\")\\n\\n - list_bigquery_tables(\\"dataset_id=analytics_250021309\\")\\n\\n - get_table_schema(\\"table_id=events_20210713, dataset_id=analytics_250021309\\")\\n\\n - create_sql_query(\\"question=Give me the total sessions per traffic source over all dates.  Tables are ga4 export format, so use traffic_source.source and count sessions, which is done by counting any event., table_info={\\\\\\"dataset\\\\\\": \\\\\\"analytics_250021309\\\\\\", \\\\\\"table\\\\\\": \\\\\\"events*\\\\\\", \\\\\\"schema\\\\\\": {\\\\\\"traffic_source\\\\\\": {\\\\\\"fields\\\\\\": {\\\\\\"medium\\\\\\": {\\\\\\"traffic_source.medium\\\\\\": {\\\\\\"type\\\\\\": \\\\\\"STRING\\\\\\", \\\\\\"mode\\\\\\": \\\\\\"NULLABLE\\\\\\"}}, \\\\\\"source\\\\\\": {\\\\\\"traffic_source.source\\\\\\": {\\\\\\"type\\\\\\": \\\\\\"STRING\\\\\\", \\\\\\"mode\\\\\\": \\\\\\"NULLABLE\\\\\\"}}, \\\\\\"name\\\\\\": {\\\\\\"traffic_source.name\\\\\\": {\\\\\\"type\\\\\\": \\\\\\"STRING\\\\\\", \\\\\\"mode\\\\\\": \\\\\\"NULLABLE\\\\\\"}}}, \\\\\\"type\\\\\\": \\\\\\"RECORD\\\\\\", \\\\\\"mode\\\\\\": \\\\\\"NULLABLE\\\\\\"}, \\\\\\"event_name\\\\\\": {\\\\\\"type\\\\\\": \\\\\\"STRING\\\\\\", \\\\\\"mode\\\\\\": \\\\\\"NULLABLE\\\\\\"}}}\\")\\n\\n - execute_sql_query(\\"query=SELECT traffic_source.source, count(*) AS session_count FROM `analytics_250021309.events_*` GROUP BY 1\\")\\n\\n - decide_to_go_on(\\"go_on=False, chat_summary=Reported the traffic per source. High confidence. The source field has null values, asked the user to confirm if this is expected. Ending dialog.\\")\\n ```\\n\\nThis is just an example of applying congitive design to an agent.  For Bertha 3.0+ a lot of more sophisticated steps can be applied, which I leave as an exercise for the reader.  A few directions you could take:\\n\\n* Add lots of examples on common GA4 BigQuery SQL to the generation agents\\n* Add memory via a vector store to keep a chat history of the good responses\\n* A lot more prompt engineering in the function docstrings and system instructions to encourage behaviour\\n* Adding a python execution bot to generate plots and data analysis\\n\\nEnjoy your cognitive designing :)\\n\\n## Poets talking to databases\\n\\nI read recently [Nick Bostrom\'s Super Intelligence](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) which included this definition of AGI:\\n\\n> Any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\\n\\nThe book was written before LLMs became a big deal so some of the forecasts are already dated but one aspect that stood out for me was Bostrom\'s definitions of how an AGI may be better than human intellect:\\n\\n1. *Very Smart* - is actively being worked on with cutting edge large language and multi-modal models.  I am skeptical they will ever be human beating in innovation for new unseen problems, but they are undoubtably going to be extremely useful by digesting all of human knowledge and presenting combinations of existing data in new and unique ways.\\n2. *Very Fast* - we have this today.  A great strength of models is that they can digest text very quickly and produce convincing summaries etc. \\n3. *Very Parallel* - we can create this today via data engineering.  With models such as [Gemini Flash 1.5 8B](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/) costing $10 for 1 billion tokens, including video and images, its possible to send 1000s of parallel calls and get back smart responses.  \\n\\nI argue that cognitive designers can work with the above three aspects to produce incredibly useful applications today, particularly with audio, images and video that are new enough to not have had their potential realised yet.  If the models are frozen in their abilities right now, we have 5-10 years of applications that can be created and be potentially ground breaking.\\n\\nBut given recent trends, its reasonable to say that the models we will have in two years time are going to be at least 10 times faster, be able to complete 2-3 more difficult tasks and be 50 times cheaper.  In that environment and with established frameworks for easy cognitive design, I hope to see great artists rise in their application. A soundbite I\'ve used is that I think STEM students and software engineers will not be the best placed to tease out performance from the latent space of these models, as its emergent properties that we are witnessing the birth of applications for. It will be more poets and philosophers who will be better placed to interact with data that constitutes all of human expression, once petty things such as code syntax is abstracted away.  I look forward to seeing what they come up with."}]}}')}}]);